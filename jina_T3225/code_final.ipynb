{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f99b59",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ec0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc77d4",
   "metadata": {},
   "source": [
    "#### seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2bca15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb210771",
   "metadata": {},
   "source": [
    "## 미스라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "def isFileChanged(file_path): # check if file changed based on last modified time \n",
    "    if os.path.getmtime(file_path) > 1614865992.0: \n",
    "        return True \n",
    "\n",
    "    return False\n",
    "\n",
    "def correct_mask_status(data_dir, invalid_list, correct_list):\n",
    "    print(\"=\"*50)\n",
    "    print(\"Change Mask Status\")\n",
    "\n",
    "    for folder in invalid_list: \n",
    "        image_dir = os.path.join(data_dir, folder)\n",
    "\n",
    "        incorrect_file = os.path.join(image_dir, correct_list[0])\n",
    "        normal_file = os.path.join(image_dir, correct_list[1])\n",
    "\n",
    "        if not isFileChanged(incorrect_file): #last modified time\n",
    "            temp = os.path.join(image_dir, correct_list[2])\n",
    "\n",
    "            os.rename(incorrect_file, temp)\n",
    "            os.rename(normal_file, incorrect_file)\n",
    "            os.rename(temp, normal_file)  # temp.jpg is not created in this folder\n",
    "\n",
    "            print(\"Changed File Names\")\n",
    "\n",
    "        else :\n",
    "            print(\"Already Changed\")\n",
    "    \n",
    "    print(\"Process Done\")\n",
    "\n",
    "\n",
    "\n",
    "def change_incorrect_gender(incorrect, src, target): \n",
    "    changed_path = incorrect.replace(src, target)\n",
    "    print(f\"{incorrect.split('images/')[1]} is changing into {target}\")\n",
    "    shutil.move(incorrect, changed_path)\n",
    "\n",
    "\n",
    "def correct_gender_status(data_dir, invalid_id_list): \n",
    "    df = pd.read_csv('/opt/ml/input/data/train/train.csv')\n",
    "\n",
    "    path_list = [] # list that contains incorrect file path\n",
    "    correct_gender_list = [] # list that contains correct gender of incorrect file path\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(\"Change Gender Status\")\n",
    "\n",
    "    for path in df['path']:\n",
    "        for pid in invalid_id_list:\n",
    "            if pid in path:\n",
    "                path_list.append(path)\n",
    "                correct_gender_list.append(\"male\" if \"female\" in path else \"female\")\n",
    "\n",
    "    for idx, foldername in enumerate(path_list):\n",
    "        folder_dir = os.path.join(data_dir, foldername)\n",
    "        gender = correct_gender_list[idx]\n",
    "\n",
    "        if gender == \"male\" and os.path.exists(folder_dir):\n",
    "            change_incorrect_gender(folder_dir, \"female\", gender)\n",
    "\n",
    "        elif gender==\"female\" and os.path.exists(folder_dir):\n",
    "            change_incorrect_gender(folder_dir, \"male\", gender)\n",
    "\n",
    "    print(\"Process Done\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def readCurrentFolders(data_dir): # change age task should read current folders\n",
    "    return sorted(list(filter(lambda p:not p.startswith(\".\"), os.listdir(data_dir))))    \n",
    "\n",
    "\n",
    "def correct_age_status(data_dir, invalid_age_id):\n",
    "    current_folders = readCurrentFolders(data_dir)\n",
    "    idx = 0 # for age dict\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(\"Change Age Status\")\n",
    "\n",
    "    for folder in current_folders: #sorted folders list\n",
    "        invalid_path = os.path.join(data_dir, folder)\n",
    "        invalid_age = folder.split(\"Asian_\")[1]\n",
    "\n",
    "        if folder.split(\"_\")[0] in invalid_age_id.keys():\n",
    "            correct_age = str(list(invalid_age_id.values())[idx])\n",
    "            correct_path = invalid_path.replace(invalid_age, correct_age)\n",
    "\n",
    "            print(f\"{invalid_path.split('images/')[1]} is changing into {correct_path.split('images/')[1]}\")\n",
    "            shutil.move(invalid_path, correct_path)\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "\n",
    "    print(\"Process Done\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     data_dir= \"/opt/ml/input/data/train/images\"\n",
    "\n",
    "#     mask_status_invalid = [\"000020_female_Asian_50\", \"004418_male_Asian_20\", \"005227_male_Asian_22\"]\n",
    "#     mask_status_name = [\"incorrect_mask.jpg\", \"normal.jpg\", \"temp.jpg\"]\n",
    "\n",
    "#     gender_status_invalid = [\"000225\", \"000664\", \"000767\", \"001498-1\", \"001509\", \"003113\", \"003223\", \"004281\", \n",
    "#     \"004432\", \"005223\", \"006359\", \"006360\",\"006361\", \"006362\", \"006363\", \"006364\", \"006424\"]\n",
    "\n",
    "#     age_status_invalid = {\"001009\" : 20, \"001064\": 20, \"001637\":20, \"001666\":20, \"001852\":20, \"004348\": 60}\n",
    "\n",
    "#     correct_mask_status(data_dir, mask_status_invalid, mask_status_name)\n",
    "#     correct_gender_status(data_dir, gender_status_invalid) \n",
    "#     correct_age_status(data_dir, age_status_invalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f2113",
   "metadata": {},
   "source": [
    "## augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = '/opt/ml/input/data/train/images'\n",
    "aug_path = '/opt/ml/input/data/train/augmentation'\n",
    "\n",
    "for subdir in os.listdir(train_folder):\n",
    "    # \".\"으로 시작하는 폴더면 건너뜀\n",
    "    if subdir.startswith(\".\"):\n",
    "        continue\n",
    "\n",
    "    # '_'기준으로 나누기\n",
    "    # splitby = subdir.split('_')\n",
    "\n",
    "    # 나이 60 이상인 것만 골라내기\n",
    "    # split_age = int(splitby[3])\n",
    "    # if split_age < 60:\n",
    "    #     continue \n",
    "    \n",
    "    subdir_path = os.path.join(train_folder, subdir)\n",
    "    \n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        # '.'로 시작하지 않는 이미지에 대해서 탐색\n",
    "        if not file_name.startswith(\".\") and not file_name.startswith(\"mask\"):\n",
    "            image_path = os.path.join(subdir_path, file_name)\n",
    "            \n",
    "            img = Image.open(image_path)\n",
    "            img = transforms.Compose([\n",
    "                transforms.CenterCrop((256, 256)) # transform 기법\n",
    "            ])(img)\n",
    "            \n",
    "            # 새로 이미지를 저장할 폴더\n",
    "            new_dir = '1' + subdir\n",
    "            new_folder = os.path.join(aug_path, new_dir)\n",
    "\n",
    "            if os.path.isdir(new_folder) == False:\n",
    "                os.mkdir(new_folder)\n",
    "            \n",
    "            img.save(os.path.join(new_folder, file_name))\n",
    "            # print(os.path.join(new_folder, file_name))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = '/opt/ml/input/data/train/images'\n",
    "aug_path = '/opt/ml/input/data/train/augmentation'\n",
    "\n",
    "for subdir in os.listdir(train_folder):\n",
    "    # \".\"으로 시작하는 폴더면 건너뜀\n",
    "    if subdir.startswith(\".\"):\n",
    "        continue\n",
    "\n",
    "    # '_'기준으로 나누기\n",
    "    splitby = subdir.split('_')\n",
    "\n",
    "    # 나이 60 이상인 것만 골라내기\n",
    "    split_age = int(splitby[3])\n",
    "    if split_age < 60:\n",
    "        continue \n",
    "    \n",
    "    subdir_path = os.path.join(train_folder, subdir)\n",
    "    \n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        # '.'로 시작하지 않는 이미지에 대해서 탐색\n",
    "        if not file_name.startswith(\".\"):\n",
    "            image_path = os.path.join(subdir_path, file_name)\n",
    "            \n",
    "            img = Image.open(image_path)\n",
    "            img = transforms.Compose([\n",
    "                transforms.ColorJitter(0.5, 0.5, 0.5, 0.5), # transform 기법\n",
    "            ])(img)\n",
    "            \n",
    "            # 새로 이미지를 저장할 폴더\n",
    "            new_dir = '2' + subdir\n",
    "            new_folder = os.path.join(aug_path, new_dir)\n",
    "\n",
    "            if os.path.isdir(new_folder) == False:\n",
    "                os.mkdir(new_folder)\n",
    "            \n",
    "            img.save(os.path.join(new_folder, file_name))\n",
    "            # print(os.path.join(new_folder, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76126446",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = '/opt/ml/input/data/train/images'\n",
    "aug_path = '/opt/ml/input/data/train/augmentation'\n",
    "\n",
    "for subdir in os.listdir(train_folder):\n",
    "    # \".\"으로 시작하는 폴더면 건너뜀\n",
    "    if subdir.startswith(\".\"):\n",
    "        continue\n",
    "\n",
    "    # '_'기준으로 나누기\n",
    "    splitby = subdir.split('_')\n",
    "\n",
    "    # 나이 60 이상인 것만 골라내기\n",
    "    split_age = int(splitby[3])\n",
    "    if split_age < 60:\n",
    "        continue \n",
    "    \n",
    "    subdir_path = os.path.join(train_folder, subdir)\n",
    "    \n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        # '.'로 시작하지 않는 이미지에 대해서 탐색\n",
    "        if not file_name.startswith(\".\"):\n",
    "            image_path = os.path.join(subdir_path, file_name)\n",
    "            \n",
    "            img = Image.open(image_path)\n",
    "            img = transforms.Compose([\n",
    "                transforms.RandomRotation(10) # transform 기법\n",
    "            ])(img)\n",
    "            \n",
    "            # 새로 이미지를 저장할 폴더\n",
    "            new_dir = '3' + subdir\n",
    "            new_folder = os.path.join(aug_path, new_dir)\n",
    "\n",
    "            if os.path.isdir(new_folder) == False:\n",
    "                os.mkdir(new_folder)\n",
    "            \n",
    "            img.save(os.path.join(new_folder, file_name))\n",
    "            # print(os.path.join(new_folder, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d150880",
   "metadata": {},
   "source": [
    "## train_image.csv 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f60d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 별 클래스를 계산해주기 위한 dictionary\n",
    "class_number = {\n",
    "    'mask':0, 'inco':6, 'norm':12,\n",
    "    'male':0, 'female':3 \n",
    "    }\n",
    "\n",
    "# 이미지 폴더들이 담긴 폴더 train_folder\n",
    "train_folder = '/opt/ml/input/data/train/images'\n",
    "aug_path = '/opt/ml/input/data/train/augmentation'\n",
    "\n",
    "# augmentation 이미지도 탐색하기 위해 리스트로 담아줌\n",
    "image_lst = [train_folder, aug_path]\n",
    "\n",
    "# csv로 만들어주기 위한 리스트 image_names\n",
    "image_names = []\n",
    "for folder in image_lst:\n",
    "    # if folder == aug_path:\n",
    "    #     continue\n",
    "    # train_folder의 폴더들 subdir별로 이미지 탐색 \n",
    "    for subdir in os.listdir(folder):\n",
    "        # \".\"으로 시작하는 폴더면 건너뜀\n",
    "        if subdir.startswith(\".\"):\n",
    "            continue\n",
    "        \n",
    "        # subdir을 \"_\"로 split해서 클래스 이름별로 나누기\n",
    "        name_class = subdir.split('_')\n",
    "        # 이미지가 담긴 폴더의 클래스 label 먼저 계산(성별, 나이)\n",
    "        folder_class = 0\n",
    "        # 성별에 따른 숫자 더해줌\n",
    "        folder_class += class_number[name_class[1]]\n",
    "        \n",
    "        # 나이는 30, 60 기준으로 나눠서 더해줌\n",
    "        age = int(name_class[3]) \n",
    "        if age < 30:\n",
    "            folder_class += 0\n",
    "        elif 30 <= age < 59:\n",
    "            folder_class += 1\n",
    "        elif 59 <= age:\n",
    "            folder_class += 2\n",
    "        \n",
    "        subdir_path = os.path.join(folder, subdir)\n",
    "\n",
    "        # subdir 안의 이미지 탐색해서 이미지 별 클래스 계산\n",
    "        for file_name in os.listdir(subdir_path):\n",
    "            # '.'로 시작하지 않는 이미지에 대해서 탐색\n",
    "            if not file_name.startswith(\".\"):\n",
    "                # image_class는 이미지 별 클래스 (마스크 여부에 따라 folder_class에 더해줌)\n",
    "                image_class = folder_class + class_number[file_name[:4]]\n",
    "                image_path = os.path.join(subdir_path, file_name)\n",
    "                # image_names에 정보 담기\n",
    "                image_names.append([name_class[0], name_class[1] , name_class[2], age, image_path, image_class])\n",
    "\n",
    "print(os.listdir(aug_path)[:3])\n",
    "print(image_names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# image_names를 id 순으로 정렬\n",
    "image_names.sort()\n",
    "# 분류 기준 더해주기\n",
    "image_names = [['id','gender','race','age','path', 'class']] + image_names\n",
    "\n",
    "# train_image.csv 파일로 만들어서 저장\n",
    "with open('train_image.csv', 'w') as file:\n",
    "    write = csv.writer(file)\n",
    "    write.writerows(image_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5a5f7",
   "metadata": {},
   "source": [
    "## train_image 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cef1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/opt/ml/code/train_image.csv\"\n",
    "train_image = pd.read_csv(csv_path)\n",
    "# 무작위로 5개 뽑아보기\n",
    "print(train_image.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f945a564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27002 entries, 0 to 27001\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      27002 non-null  object\n",
      " 1   gender  27002 non-null  object\n",
      " 2   race    27002 non-null  object\n",
      " 3   age     27002 non-null  int64 \n",
      " 4   path    27002 non-null  object\n",
      " 5   class   27002 non-null  int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_image.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_image.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96197a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAXyElEQVR4nO3dfbRddX3n8ffHRPDZgNwymMRJWlKcwOiI4UGZdim4IDjW4BQFxpFo0bQjPnSsT9TVMkuko1NnUFpBM5IhdLlASlFiRdMUUKYqD0EUBKRkQCQRJRpAR0ZYge/8cX6RY7w33Ozccw6X+36tddbd+7t/e+/fXuusfLL3b5+9U1VIktTFk0bdAUnS9GWISJI6M0QkSZ0ZIpKkzgwRSVJns0fdgWHba6+9asGCBaPuhiRNK9ddd92Pq2ps+/qMC5EFCxawfv36UXdDkqaVJHeOV/dyliSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpsxn3i/VdMXf+8/jBxrtG3Q09QT133nw23fX9UXdD2ikDC5Ekq4BXAfdU1QF99bcDJwMPA1+sqve2+inASa3+jqpa2+pLgY8Ds4BPV9WHW30hcAHwHOA64A1V9dCgjgfgBxvv4rhPfX2Qu9AM9tk/fOmouyDttEFezjoXWNpfSPJyYBnwwqraH/hoqy8Gjgf2b+uclWRWklnAJ4CjgcXACa0twEeAM6pqX+BeegEkSRqigYVIVV0JbNmu/J+AD1fVg63NPa2+DLigqh6sqjuADcDB7bOhqm5vZxkXAMuSBDgcuKitvxo4ZlDHIkka37AH1n8b+J0kVyf5apKDWn0u0D/YsLHVJqo/B7ivqrZuVx9XkhVJ1idZv3nz5ik6FEnSsENkNrAncCjwHuDCdlYxUFW1sqqWVNWSsbFfexy+JKmjYd+dtRG4uKoKuCbJI8BewCZgfl+7ea3GBPWfAHOSzG5nI/3tJUlDMuwzkc8DLwdI8tvAbsCPgTXA8Ul2b3ddLQKuAa4FFiVZmGQ3eoPva1oIXQEc27a7HLhkqEciSRroLb7nAy8D9kqyETgVWAWsSvId4CFgeQuEm5JcCNwMbAVOrqqH23beBqyld4vvqqq6qe3ifcAFST4EXA+cM6hjkSSNb2AhUlUnTLDoP07Q/nTg9HHqlwKXjlO/nd7dW5KkEfGxJ5KkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0NLESSrEpyT3uL4fbL/iRJJdmrzSfJmUk2JLkhyYF9bZcnua19lvfVX5zkxrbOmUkyqGORJI1vkGci5wJLty8mmQ8cCXy/r3w0vfeqLwJWAGe3tnvSe63uIfTeYnhqkj3aOmcDb+lb79f2JUkarIGFSFVdCWwZZ9EZwHuB6qstA86rnquAOUn2AY4C1lXVlqq6F1gHLG3LnlVVV7V3tJ8HHDOoY5EkjW+oYyJJlgGbqurb2y2aC9zVN7+x1XZU3zhOfaL9rkiyPsn6zZs378IRSJL6DS1EkjwN+FPgz4e1z22qamVVLamqJWNjY8PevSQ9YQ3zTOS3gIXAt5N8D5gHfDPJvwA2AfP72s5rtR3V541TlyQN0dBCpKpurKrfqKoFVbWA3iWoA6vqh8Aa4MR2l9ahwP1VdTewFjgyyR5tQP1IYG1b9tMkh7a7sk4ELhnWsUiSegZ5i+/5wDeA/ZJsTHLSDppfCtwObAD+J/BWgKraApwGXNs+H2w1WptPt3X+D/ClQRyHJGliswe14ao64TGWL+ibLuDkCdqtAlaNU18PHLBrvZQk7Qp/sS5J6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktTZIN9suCrJPUm+01f7yyTfTXJDks8lmdO37JQkG5LcmuSovvrSVtuQ5P199YVJrm71zybZbVDHIkka3yDPRM4Flm5XWwccUFUvAP4ZOAUgyWLgeGD/ts5ZSWYlmQV8AjgaWAyc0NoCfAQ4o6r2Be4FdvT6XUnSAAwsRKrqSmDLdrV/qKqtbfYqYF6bXgZcUFUPVtUd9N6bfnD7bKiq26vqIeACYFmSAIcDF7X1VwPHDOpYJEnjG+WYyB8AX2rTc4G7+pZtbLWJ6s8B7usLpG11SdIQjSREknwA2Ap8Zkj7W5FkfZL1mzdvHsYuJWlGGHqIJHkj8Crg9VVVrbwJmN/XbF6rTVT/CTAnyezt6uOqqpVVtaSqloyNjU3JcUiShhwiSZYC7wVeXVUP9C1aAxyfZPckC4FFwDXAtcCidifWbvQG39e08LkCOLatvxy4ZFjHIUnqGeQtvucD3wD2S7IxyUnAXwPPBNYl+VaSTwJU1U3AhcDNwJeBk6vq4Tbm8TZgLXALcGFrC/A+4F1JNtAbIzlnUMciSRrf7Mdu0k1VnTBOecJ/6KvqdOD0ceqXApeOU7+d3t1bkqQR8RfrkqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSps4E9O0vSTnrSbHov7ZSm3nPnzWfTXd+f8u0aItLjxSNbOe5TXx91L/QE9dk/fOlAtuvlLElSZ4aIJKkzQ0SS1JkhIknqbJCvx12V5J4k3+mr7ZlkXZLb2t89Wj1JzkyyIckNSQ7sW2d5a39bkuV99RcnubGtc2a8rUWShm6QZyLnAku3q70fuKyqFgGXtXmAo4FF7bMCOBt6oQOcChxC71W4p24LntbmLX3rbb8vSdKADSxEqupKYMt25WXA6ja9Gjimr35e9VwFzEmyD3AUsK6qtlTVvcA6YGlb9qyquqqqCjivb1uSpCEZ9pjI3lV1d5v+IbB3m54L3NXXbmOr7ai+cZz6uJKsSLI+yfrNmzfv2hFIkn5pZAPr7QyihrSvlVW1pKqWjI2NDWOXkjQjDDtEftQuRdH+3tPqm4D5fe3mtdqO6vPGqUuShmjYIbIG2HaH1XLgkr76ie0urUOB+9tlr7XAkUn2aAPqRwJr27KfJjm03ZV1Yt+2JElDMrBnZyU5H3gZsFeSjfTusvowcGGSk4A7gde15pcCrwQ2AA8AbwKoqi1JTgOube0+WFXbBuvfSu8OsKcCX2ofSdIQDSxEquqECRYdMU7bAk6eYDurgFXj1NcDB+xKHyVJu2ZSl7OSHDaZmiRpZpnsmMhfTbImSZpBdng5K8lLgJcCY0ne1bfoWcCsQXZMkvT491hjIrsBz2jtntlX/ylw7KA6JUmaHnYYIlX1VeCrSc6tqjuH1CdJ0jQx2buzdk+yEljQv05VHT6ITkmSpofJhsjfAp8EPg08PLjuSJKmk8mGyNaqOnugPZEkTTuTvcX3C0nemmSf9mKpPdu7PiRJM9hkz0S2Pe/qPX21An5zarsjSZpOJhUiVbVw0B2RJE0/kwqRJCeOV6+q86a2O5Kk6WSyl7MO6pt+Cr2HKH6T3mtpJUkz1GQvZ729fz7JHOCCgfRIkjRtdH0p1c8Bx0kkaYab7JjIF3j0feizgH8FXDioTkmSpofJjol8tG96K3BnVW3sutMk/xl4M71gupHemwz3oXeJ7DnAdcAbquqhJLvTG3t5MfAT4Liq+l7bzinASfR+Rf+OqlrbtU+SpJ03qctZ7UGM36X3JN89gIe67jDJXOAdwJKqOoDemc3xwEeAM6pqX+BeeuFA+3tvq5/R2pFkcVtvf2ApcFYSH08vSUM02Tcbvg64BngtvfeiX51kVx4FPxt4apLZwNOAu4HDgYva8tXAMW16WZunLT8iSVr9gqp6sKruoPd+9oN3oU+SpJ002ctZHwAOqqp7AJKMAf/Io//oT1pVbUryUeD7wP8D/oHe5av7qmpra7YRmNum5wJ3tXW3Jrmf3iWvucBVfZvuX+dXJFkBrAB43vOet7NdliRNYLJ3Zz1pW4A0P9mJdX9Fkj3onUUsBJ4LPJ3e5aiBqaqVVbWkqpaMjY0NcleSNKNM9kzky0nWAue3+eOASzvu8xXAHVW1GSDJxcBhwJwks9vZyDxgU2u/CZgPbGyXv55NL8S21bfpX0eSNAQ7PJtIsm+Sw6rqPcCngBe0zzeAlR33+X3g0CRPa2MbRwA3A1fw6Ct3lwOXtOk1PPoAyGOBy6uqWv34JLsnWQgsojduI0kaksc6E/kYcApAVV0MXAyQ5F+3Zb+3szusqquTXETvsSlbgevpBdIXgQuSfKjVzmmrnAP8TZINwBZ6d2RRVTcluZBeAG0FTq4qX5glSUP0WCGyd1XduH2xqm5MsqDrTqvqVODU7cq3M87dVVX1C3p3hY23ndOB07v2Q5K0ax5rcHzODpY9dSo7Ikmafh4rRNYnecv2xSRvpndbriRpBnusy1l/DHwuyet5NDSWALsBrxlkxyRJj387DJGq+hHw0iQvBw5o5S9W1eUD75kk6XFvsu8TuYLeLbiSJP1S1/eJSJJkiEiSujNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM5GEiJJ5iS5KMl3k9yS5CVJ9kyyLslt7e8erW2SnJlkQ5IbkhzYt53lrf1tSZZPvEdJ0iCM6kzk48CXq+r5wAuBW4D3A5dV1SLgsjYPcDSwqH1WAGcDJNmT3it2D6H3Wt1TtwWPJGk4hh4iSZ4N/C5wDkBVPVRV9wHLgNWt2WrgmDa9DDiveq4C5iTZBzgKWFdVW6rqXmAdsHSIhyJJM94ozkQWApuB/5Xk+iSfTvJ0YO+quru1+SGwd5ueC9zVt/7GVpuo/muSrEiyPsn6zZs3T+GhSNLMNooQmQ0cCJxdVS8Cfs6jl64AqKoCaqp2WFUrq2pJVS0ZGxubqs1K0ow3ihDZCGysqqvb/EX0QuVH7TIV7e89bfkmYH7f+vNabaK6JGlIhh4iVfVD4K4k+7XSEcDNwBpg2x1Wy4FL2vQa4MR2l9ahwP3tstda4Mgke7QB9SNbTZI0JJN6x/oAvB34TJLdgNuBN9ELtAuTnATcCbyutb0UeCWwAXigtaWqtiQ5Dbi2tftgVW0Z3iFIkkYSIlX1LWDJOIuOGKdtASdPsJ1VwKqp7Z0kabL8xbokqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnIwuRJLOSXJ/k79v8wiRXJ9mQ5LPtrYck2b3Nb2jLF/Rt45RWvzXJUaM5EkmauUZ5JvJO4Ja++Y8AZ1TVvsC9wEmtfhJwb6uf0dqRZDFwPLA/sBQ4K8msIfVdksSIQiTJPODfAZ9u8wEOBy5qTVYDx7TpZW2etvyI1n4ZcEFVPVhVd9B7B/vBwzkCSRKM7kzkY8B7gUfa/HOA+6pqa5vfCMxt03OBuwDa8vtb+1/Wx1nnVyRZkWR9kvWbN2+eyuOQpBlt6CGS5FXAPVV13bD2WVUrq2pJVS0ZGxsb1m4l6Qlv9gj2eRjw6iSvBJ4CPAv4ODAnyex2tjEP2NTabwLmAxuTzAaeDfykr75N/zqSpCEY+plIVZ1SVfOqagG9gfHLq+r1wBXAsa3ZcuCSNr2mzdOWX15V1erHt7u3FgKLgGuGdBiSJEZzJjKR9wEXJPkQcD1wTqufA/xNkg3AFnrBQ1XdlORC4GZgK3ByVT08/G5L0sw10hCpqq8AX2nTtzPO3VVV9QvgtROsfzpw+uB6KEnaEX+xLknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1NnQQyTJ/CRXJLk5yU1J3tnqeyZZl+S29nePVk+SM5NsSHJDkgP7trW8tb8tyfKJ9ilJGoxRnIlsBf6kqhYDhwInJ1kMvB+4rKoWAZe1eYCj6b0/fRGwAjgbeqEDnAocQu+NiKduCx5J0nAMPUSq6u6q+mab/hlwCzAXWAasbs1WA8e06WXAedVzFTAnyT7AUcC6qtpSVfcC64ClQzwUSZrxRjomkmQB8CLgamDvqrq7LfohsHebngvc1bfaxlabqD7eflYkWZ9k/ebNm6es/5I0040sRJI8A/g74I+r6qf9y6qqgJqqfVXVyqpaUlVLxsbGpmqzkjTjjSREkjyZXoB8pqoubuUftctUtL/3tPomYH7f6vNabaK6JGlIRnF3VoBzgFuq6n/0LVoDbLvDajlwSV/9xHaX1qHA/e2y11rgyCR7tAH1I1tNkjQks0ewz8OANwA3JvlWq/0p8GHgwiQnAXcCr2vLLgVeCWwAHgDeBFBVW5KcBlzb2n2wqrYM5xAkSTCCEKmqfwIyweIjxmlfwMkTbGsVsGrqeidJ2hn+Yl2S1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1Nm0D5EkS5PcmmRDkvePuj+SNJNM6xBJMgv4BHA0sBg4Icni0fZKkmaOaR0iwMHAhqq6vaoeAi4Alo24T5I0Y6SqRt2HzpIcCyytqje3+TcAh1TV27ZrtwJY0Wb3A24dakdnrr2AH4+6E3rC8vs1XP+yqsa2L84eRU+GrapWAitH3Y+ZJsn6qloy6n7oicnv1+PDdL+ctQmY3zc/r9UkSUMw3UPkWmBRkoVJdgOOB9aMuE+SNGNM68tZVbU1yduAtcAsYFVV3TTibulRXkLUIPn9ehyY1gPrkqTRmu6XsyRJI2SISJI6M0Q0oSTvSHJLks8MaPv/Jcm7B7FtzTxJXpbk70fdj5lmWg+sa+DeCryiqjaOuiOSHp88E9G4knwS+E3gS0k+kGRVkmuSXJ9kWWvzxiSfT7IuyfeSvC3Ju1qbq5Ls2dq9Jcm1Sb6d5O+SPG2c/f1Wki8nuS7J/07y/OEesR4PkixI8t0k5yb55ySfSfKKJF9LcluSg9vnG+179vUk+42znaeP953V1DNENK6q+iPgB8DLgacDl1fVwW3+L5M8vTU9APj3wEHA6cADVfUi4BvAia3NxVV1UFW9ELgFOGmcXa4E3l5VLwbeDZw1mCPTNLAv8N+B57fPfwD+Lb3vxZ8C3wV+p33P/hz4i3G28QEm/s5qCnk5S5NxJPDqvvGLpwDPa9NXVNXPgJ8luR/4QqvfCLygTR+Q5EPAHOAZ9H7X80tJngG8FPjbJNvKuw/iQDQt3FFVNwIkuQm4rKoqyY3AAuDZwOoki4ACnjzONib6zt4y6M7PNIaIJiPA71fVrzy4MskhwIN9pUf65h/h0e/XucAxVfXtJG8EXrbd9p8E3FdV/2Zqu61p6rG+U6fR+8/La5IsAL4yzjbG/c5q6nk5S5OxFnh72mlCkhft5PrPBO5O8mTg9dsvrKqfAnckeW3bfpK8cBf7rCeuZ/PoM/LeOEGbXf3OapIMEU3GafQuGdzQLi+ctpPr/xlwNfA1etezx/N64KQk3wZuwvfCaGL/DfivSa5n4qspu/qd1ST52BNJUmeeiUiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0R6HGvPkDp21P2QJmKISE8gSXwKhYbKEJGmSJI/S3Jrkn9Kcn6Sd0/0dOJ2hnFmewrt7dvONtqv9f+6becfgd/o2/6Lk3y1bWttkn1a/StJPpZkPfDOURy7Zi7/1yJNgSQHAb8PvJDeL6W/CVxH7+nEf1RVt7VnjZ0FHN5W24fe02mfD6wBLgJeA+wHLAb2Bm4GVrVHxvwVsKyqNic5jt5Tk/+gbWu3qloy8AOVtmOISFPjMOCSqvoF8IskX6D35NgdPZ3481X1CHBzkr1b7XeB86vqYeAHSS5v9f3oPXZ/XdvWLODuvm19dgDHJD0mQ0QanMd6OnH/02ozQZv+5TdV1UsmWP7zne2cNBUcE5GmxteA30vylPZ+lFcBD7DzTye+Ejguyaw25vHyVr8VGEvykratJyfZfyBHIu0EQ0SaAlV1Lb1xjRuAL9F7Kdf97PzTiT8H3EZvLOQ8em+IpKoeAo4FPtK29S16l8qkkfIpvtIUSfKMqvq/7R3yVwIrquqbo+6XNEiOiUhTZ2WSxfQG1FcbIJoJPBORJHXmmIgkqTNDRJLUmSEiSerMEJEkdWaISJI6+/8Pb861/f8S1QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# class 별 분포 살펴보기\n",
    "sns.histplot(x='class',data=train_image,)\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(x='age', data=train_image,)\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(x='gender', data=train_image,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e25cb",
   "metadata": {},
   "source": [
    "#### 픽셀 별 평균, 표준편차 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "58226e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55912894 0.5197964 0.49549162\n",
      "0.23119642 0.23979054 0.24312052\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_image = pd.read_csv(csv_path)\n",
    "\n",
    "meanRGB = []\n",
    "stdRGB = []\n",
    "\n",
    "for p in list(train_image[\"path\"]):\n",
    "    i = Image.open(p)\n",
    "    i = transforms.Compose([ToTensor()])(i)\n",
    "    meanRGB.append(np.mean(i.numpy(), axis=(1,2)))\n",
    "    stdRGB.append(np.std(i.numpy(), axis=(1,2)))\n",
    "\n",
    "meanR = np.mean([m[0] for m in meanRGB])\n",
    "meanG = np.mean([m[1] for m in meanRGB])\n",
    "meanB = np.mean([m[2] for m in meanRGB])\n",
    "\n",
    "stdR = np.mean([s[0] for s in stdRGB])\n",
    "stdG = np.mean([s[1] for s in stdRGB])\n",
    "stdB = np.mean([s[2] for s in stdRGB])\n",
    "\n",
    "print(meanR, meanG, meanB)\n",
    "print(stdR, stdG, stdB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f2475",
   "metadata": {},
   "source": [
    "## Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4978e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        train_image = pd.read_csv(path) # dataframe으로 만들어주기\n",
    "        self.image_paths = train_image[\"path\"]\n",
    "        self.image_labels = train_image[\"class\"]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = None\n",
    "        img = Image.open(self.image_paths[idx])\n",
    "        label = torch.tensor(self.image_labels[idx])\n",
    "        if self.transform is not None:\n",
    "            img_transform = self.transform(img)\n",
    "        return img_transform, label\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c182eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset = CustomDataset(csv_path, transform=transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    Resize((384,384)),\n",
    "    ToTensor(), \n",
    "    Normalize([0.5591, 0.5197, 0.4954],[0.2312, 0.2397, 0.2431]),\n",
    "    ]))\n",
    "v_dataset = CustomDataset(csv_path, transform=transforms.Compose([\n",
    "    Resize((384,384)),\n",
    "    ToTensor(), \n",
    "    Normalize([0.5591, 0.5197, 0.4954],[0.2312, 0.2397, 0.2431])\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b592cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(t_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf2f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(v_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843c968",
   "metadata": {},
   "source": [
    "#### train, validation dataset 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878f1755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22113 4889\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# dataset 두 개로 나누기\n",
    "def split_index(length, val_ratio):\n",
    "    n_val = int(length * val_ratio)\n",
    "    val_indices = set(random.choices(range(length), k=n_val))\n",
    "    train_indices = set(range((length))) - set(val_indices)\n",
    "    return val_indices, train_indices\n",
    "\n",
    "val_indices, train_indices = split_index(len(t_dataset), val_ratio=0.2)\n",
    "train_dataset = Subset(t_dataset, list(train_indices))\n",
    "val_dataset = Subset(v_dataset, list(val_indices))\n",
    "\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c9904",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5968156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=1)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b29e2a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5de79139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13743c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "model_resnet = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_resnet.fc.in_features\n",
    "model_resnet.fc = nn.Linear(num_ftrs, 18)\n",
    "\n",
    "model_efficient = EfficientNet.from_pretrained('efficientnet-b0', num_classes=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af859eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb31980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 출력 채널 개수 (예측 class type 개수) 18\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "torch.nn.init.xavier_uniform_(model_resnet.fc.weight)\n",
    "stdv = 1. / math.sqrt(model_resnet.fc.weight.size(1))\n",
    "model_resnet.fc.bias.data.uniform_(-stdv, stdv)\n",
    "print(\"네트워크 출력 채널 개수 (예측 class type 개수)\", model_resnet.fc.weight.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e25e9f",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c943e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=3, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "\n",
    "# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\n",
    "class F1Loss(nn.Module):\n",
    "    def __init__(self, classes=18, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.ndim == 2\n",
    "        assert y_true.ndim == 1\n",
    "        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\n",
    "        return 1 - f1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebba4d1",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "535b434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4effd27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9ba3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = model_efficient\n",
    "my_model.to(device) \n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.AdamW(my_model.parameters(), lr=LEARNING_RATE) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae46c91",
   "metadata": {},
   "source": [
    "### CutMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecae07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix(images, labels):  \n",
    "    X = images\n",
    "    y = labels     \n",
    "    rand_index = torch.randperm(X.size()[0]).to(device)\n",
    "    target_a = y\n",
    "    target_b = y[rand_index]            \n",
    "    bbx1, bby1, bbx2, bby2 = 50, 148, 192, 290\n",
    "    X[:, :, bbx1:bbx2, bby1:bby2] = X[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (X.size()[-1] * X.size()[-2]))\n",
    "    outputs = my_model(X)\n",
    "    loss = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)\n",
    "    return outputs, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "loader_type = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"test\": val_dataloader\n",
    "}\n",
    "\n",
    "NUM_EPOCH = 20\n",
    "my_model.to(device)\n",
    "best_test_acc = 0\n",
    "best_test_loss = 9999.\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    \n",
    "    for mode in [\"train\", \"test\"]:\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "        \n",
    "        if mode == 'train':\n",
    "            my_model.train()\n",
    "        elif mode == 'test':\n",
    "            my_model.eval()\n",
    "        \n",
    "        for idx, (images, labels) in enumerate(loader_type[mode]):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(mode == 'train'):\n",
    "                # if np.random.random() > 0.7 : # cutmix 실행 확률\n",
    "                #     logits, loss = cutmix(images, labels)\n",
    "\n",
    "                # else:    \n",
    "                #     logits = my_model(images)\n",
    "                #     loss = criterion(logits, labels)\n",
    "                \n",
    "                logits = my_model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "                _, preds = torch.max(logits, 1)\n",
    "                \n",
    "                if mode == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_acc += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(loader_type[mode].dataset)\n",
    "        epoch_acc = running_acc / len(loader_type[mode].dataset)\n",
    "        \n",
    "        print(f'epoch: {epoch+1}, {mode}-데이터 셋 Loss: {epoch_loss:.3f}, acc: {epoch_acc:.3f}')\n",
    "        if mode == 'test' and best_test_acc < epoch_acc:\n",
    "              best_test_acc = epoch_acc\n",
    "        if mode == 'test' and best_test_loss > epoch_loss:\n",
    "              best_test_loss = epoch_loss\n",
    "        elif mode == 'test' and best_test_loss < epoch_loss:\n",
    "            patience += 1\n",
    "    \n",
    "    if patience == 4:\n",
    "        print('finished by early stopping!!')\n",
    "        break\n",
    "        \n",
    "print(\"training end!!\")\n",
    "print(f\"best acc: {best_test_acc}, best loss: {best_test_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209dd3ef",
   "metadata": {},
   "source": [
    "## Test Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d5aed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81733001",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval'\n",
    "\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "test_dataset = TestDataset(image_paths, transform=transforms.Compose([\n",
    "    Resize((384, 384)),\n",
    "    ToTensor(), \n",
    "    Normalize([0.5591, 0.5197, 0.4954],[0.2312, 0.2397, 0.2431])\n",
    "    ]))\n",
    "test_loader = DataLoader(test_dataset,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46dc99e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c932e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "my_model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = my_model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "submission.to_csv(os.path.join(test_dir, 'submission32.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
