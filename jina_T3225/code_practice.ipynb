{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ec0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d150880",
   "metadata": {},
   "source": [
    "## train_image.csv 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f60d022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['._003456_male_Asian_56', '._000571_male_Asian_51', '006131_male_Asian_20']\n",
      "[['006131', 'male', 'Asian', 20, '/opt/ml/input/data/train/images/006131_male_Asian_20/mask2.png', 0], ['006131', 'male', 'Asian', 20, '/opt/ml/input/data/train/images/006131_male_Asian_20/mask3.png', 0], ['006131', 'male', 'Asian', 20, '/opt/ml/input/data/train/images/006131_male_Asian_20/mask5.png', 0], ['006131', 'male', 'Asian', 20, '/opt/ml/input/data/train/images/006131_male_Asian_20/mask1.png', 0], ['006131', 'male', 'Asian', 20, '/opt/ml/input/data/train/images/006131_male_Asian_20/incorrect_mask.png', 6]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class_number = {\n",
    "    'mask':0, 'inco':6, 'norm':12,\n",
    "    'male':0, 'female':3 \n",
    "    }\n",
    "\n",
    "train_folder = '/opt/ml/input/data/train/images'\n",
    "image_names = []\n",
    "for subdir in os.listdir(train_folder):\n",
    "    if subdir.startswith(\".\"):\n",
    "        continue\n",
    "    \n",
    "    name_class = subdir.split('_')\n",
    "    folder_class = 0\n",
    "    folder_class += class_number[name_class[1]]\n",
    "\n",
    "    age = int(name_class[3]) \n",
    "    if age < 30:\n",
    "        folder_class += 0\n",
    "    elif 30 <= age < 60:\n",
    "        folder_class += 1\n",
    "    elif 60 <= age:\n",
    "        folder_class += 2\n",
    "    \n",
    "    subdir_path = train_folder + '/' + subdir\n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        if not file_name.startswith(\".\"):\n",
    "            image_class = folder_class + class_number[file_name[:4]]\n",
    "            image_path = subdir_path + '/' + file_name\n",
    "            image_names.append([name_class[0], name_class[1] , name_class[2], age, image_path, image_class])\n",
    "\n",
    "print(os.listdir(train_folder)[:3])\n",
    "print(image_names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b4d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "image_names.sort()\n",
    "image_names = [['id','gender','race','age','path', 'class']] + image_names\n",
    "\n",
    "with open('train_image.csv', 'w') as file:\n",
    "    write = csv.writer(file)\n",
    "    write.writerows(image_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5a5f7",
   "metadata": {},
   "source": [
    "## train_image 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61cef1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  gender   race  age  \\\n",
      "8405  003161  female  Asian   20   \n",
      "1338  000574  female  Asian   56   \n",
      "6403  001668  female  Asian   48   \n",
      "4763  001382  female  Asian   60   \n",
      "7387  001925  female  Asian   50   \n",
      "\n",
      "                                                   path  class  \n",
      "8405  /opt/ml/input/data/train/images/003161_female_...      3  \n",
      "1338  /opt/ml/input/data/train/images/000574_female_...      4  \n",
      "6403  /opt/ml/input/data/train/images/001668_female_...      4  \n",
      "4763  /opt/ml/input/data/train/images/001382_female_...      5  \n",
      "7387  /opt/ml/input/data/train/images/001925_female_...      4  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"/opt/ml/code/train_image.csv\"\n",
    "train_image = pd.read_csv(csv_path)\n",
    "print(train_image.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f945a564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18900 entries, 0 to 18899\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      18900 non-null  object\n",
      " 1   gender  18900 non-null  object\n",
      " 2   race    18900 non-null  object\n",
      " 3   age     18900 non-null  int64 \n",
      " 4   path    18900 non-null  object\n",
      " 5   class   18900 non-null  int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 886.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_image.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d73a4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  gender   race           age  \\\n",
      "count    18900   18900  18900  18900.000000   \n",
      "unique    2699       2      1           NaN   \n",
      "top     003397  female  Asian           NaN   \n",
      "freq        14   11606  18900           NaN   \n",
      "mean       NaN     NaN    NaN     37.708148   \n",
      "std        NaN     NaN    NaN     16.983208   \n",
      "min        NaN     NaN    NaN     18.000000   \n",
      "25%        NaN     NaN    NaN     20.000000   \n",
      "50%        NaN     NaN    NaN     36.000000   \n",
      "75%        NaN     NaN    NaN     55.000000   \n",
      "max        NaN     NaN    NaN     60.000000   \n",
      "\n",
      "                                                     path         class  \n",
      "count                                               18900  18900.000000  \n",
      "unique                                              18900           NaN  \n",
      "top     /opt/ml/input/data/train/images/001482_female_...           NaN  \n",
      "freq                                                    1           NaN  \n",
      "mean                                                  NaN      5.010317  \n",
      "std                                                   NaN      4.660533  \n",
      "min                                                   NaN      0.000000  \n",
      "25%                                                   NaN      1.000000  \n",
      "50%                                                   NaN      4.000000  \n",
      "75%                                                   NaN      7.000000  \n",
      "max                                                   NaN     17.000000  \n"
     ]
    }
   ],
   "source": [
    "print(train_image.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c96197a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.histplot(x='class',data=train_image, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f2475",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4978e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path, transform, train=True):\n",
    "        train_image = pd.read_csv(path)\n",
    "        self.train = train\n",
    "        self.image_paths = train_image[\"path\"]\n",
    "        self.image_labels = train_image[\"class\"]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_labels)-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = None\n",
    "        img = Image.open(self.image_paths[idx])\n",
    "        if self.train:\n",
    "            label = torch.tensor(self.image_labels[idx])\n",
    "        img_Tensor = self.transform(img)\n",
    "        return img_Tensor, label\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c182eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(csv_path, transform=transforms.ToTensor())\n",
    "val_dataset = CustomDataset(csv_path, transform=transforms.ToTensor(), train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddb3097c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.7490, 0.7490, 0.7490,  ..., 0.7882, 0.7882, 0.7882],\n",
       "          [0.7490, 0.7490, 0.7490,  ..., 0.7882, 0.7882, 0.7882],\n",
       "          [0.7490, 0.7490, 0.7490,  ..., 0.7882, 0.7882, 0.7882],\n",
       "          ...,\n",
       "          [0.5843, 0.5882, 0.5882,  ..., 0.5922, 0.5922, 0.5922],\n",
       "          [0.5725, 0.5725, 0.5725,  ..., 0.5961, 0.5961, 0.5961],\n",
       "          [0.5608, 0.5608, 0.5608,  ..., 0.6078, 0.6078, 0.6078]],\n",
       " \n",
       "         [[0.7451, 0.7451, 0.7451,  ..., 0.7843, 0.7843, 0.7843],\n",
       "          [0.7451, 0.7451, 0.7451,  ..., 0.7843, 0.7843, 0.7843],\n",
       "          [0.7451, 0.7451, 0.7451,  ..., 0.7843, 0.7843, 0.7843],\n",
       "          ...,\n",
       "          [0.3804, 0.3843, 0.3843,  ..., 0.3686, 0.3686, 0.3686],\n",
       "          [0.3686, 0.3686, 0.3686,  ..., 0.3725, 0.3725, 0.3725],\n",
       "          [0.3569, 0.3569, 0.3569,  ..., 0.3686, 0.3686, 0.3686]],\n",
       " \n",
       "         [[0.7255, 0.7255, 0.7255,  ..., 0.7647, 0.7647, 0.7647],\n",
       "          [0.7255, 0.7255, 0.7255,  ..., 0.7647, 0.7647, 0.7647],\n",
       "          [0.7255, 0.7255, 0.7255,  ..., 0.7647, 0.7647, 0.7647],\n",
       "          ...,\n",
       "          [0.2353, 0.2392, 0.2392,  ..., 0.2510, 0.2510, 0.2510],\n",
       "          [0.2235, 0.2235, 0.2235,  ..., 0.2667, 0.2667, 0.2667],\n",
       "          [0.2118, 0.2118, 0.2118,  ..., 0.2863, 0.2863, 0.2863]]]),\n",
       " tensor(10))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5968156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=1)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e63be53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.4706, 0.4627, 0.4510,  ..., 0.4471, 0.5059, 0.4706],\n",
       "           [0.4784, 0.4627, 0.4510,  ..., 0.4745, 0.5529, 0.5608],\n",
       "           [0.4824, 0.4706, 0.4549,  ..., 0.5098, 0.5961, 0.6314],\n",
       "           ...,\n",
       "           [0.1059, 0.0824, 0.0667,  ..., 0.4980, 0.4745, 0.4314],\n",
       "           [0.1333, 0.1098, 0.0902,  ..., 0.5216, 0.4824, 0.4275],\n",
       "           [0.1373, 0.1137, 0.0980,  ..., 0.5373, 0.4980, 0.4275]],\n",
       " \n",
       "          [[0.4392, 0.4235, 0.4118,  ..., 0.4431, 0.5020, 0.4667],\n",
       "           [0.4392, 0.4235, 0.4118,  ..., 0.4706, 0.5490, 0.5569],\n",
       "           [0.4431, 0.4235, 0.4078,  ..., 0.5059, 0.5922, 0.6275],\n",
       "           ...,\n",
       "           [0.0902, 0.0863, 0.0745,  ..., 0.4314, 0.4392, 0.4235],\n",
       "           [0.1294, 0.1098, 0.1098,  ..., 0.4627, 0.4667, 0.4275],\n",
       "           [0.1373, 0.1216, 0.1176,  ..., 0.4863, 0.4824, 0.4353]],\n",
       " \n",
       "          [[0.4314, 0.4196, 0.4078,  ..., 0.4275, 0.4863, 0.4510],\n",
       "           [0.4353, 0.4196, 0.4078,  ..., 0.4549, 0.5333, 0.5412],\n",
       "           [0.4392, 0.4235, 0.4078,  ..., 0.4902, 0.5765, 0.6118],\n",
       "           ...,\n",
       "           [0.0784, 0.0667, 0.0549,  ..., 0.4039, 0.4039, 0.3765],\n",
       "           [0.1216, 0.1020, 0.0941,  ..., 0.4353, 0.4235, 0.3882],\n",
       "           [0.1294, 0.1098, 0.1020,  ..., 0.4549, 0.4392, 0.3922]]],\n",
       " \n",
       " \n",
       "         [[[0.2431, 0.2510, 0.2588,  ..., 0.8039, 0.8039, 0.8039],\n",
       "           [0.2471, 0.2510, 0.2588,  ..., 0.8039, 0.8039, 0.8039],\n",
       "           [0.2510, 0.2549, 0.2627,  ..., 0.8039, 0.8039, 0.8039],\n",
       "           ...,\n",
       "           [0.0627, 0.0745, 0.0863,  ..., 0.7176, 0.7255, 0.7333],\n",
       "           [0.0667, 0.0863, 0.0824,  ..., 0.7176, 0.7059, 0.6980],\n",
       "           [0.0549, 0.0745, 0.0784,  ..., 0.7216, 0.7020, 0.6745]],\n",
       " \n",
       "          [[0.4863, 0.4941, 0.5020,  ..., 0.8118, 0.8118, 0.8118],\n",
       "           [0.4902, 0.4941, 0.5020,  ..., 0.8118, 0.8118, 0.8118],\n",
       "           [0.4941, 0.4980, 0.5059,  ..., 0.8118, 0.8118, 0.8118],\n",
       "           ...,\n",
       "           [0.0784, 0.0824, 0.0902,  ..., 0.6510, 0.6431, 0.6471],\n",
       "           [0.1020, 0.1020, 0.0863,  ..., 0.6588, 0.6353, 0.6157],\n",
       "           [0.0941, 0.0980, 0.0863,  ..., 0.6745, 0.6392, 0.6039]],\n",
       " \n",
       "          [[0.5294, 0.5373, 0.5451,  ..., 0.8078, 0.8078, 0.8078],\n",
       "           [0.5333, 0.5373, 0.5451,  ..., 0.8078, 0.8078, 0.8078],\n",
       "           [0.5373, 0.5412, 0.5490,  ..., 0.8078, 0.8078, 0.8078],\n",
       "           ...,\n",
       "           [0.0745, 0.0706, 0.0667,  ..., 0.5725, 0.5608, 0.5647],\n",
       "           [0.0980, 0.0980, 0.0667,  ..., 0.5686, 0.5490, 0.5333],\n",
       "           [0.0902, 0.0902, 0.0667,  ..., 0.5804, 0.5490, 0.5176]]],\n",
       " \n",
       " \n",
       "         [[[0.7765, 0.7765, 0.7765,  ..., 0.3255, 0.3294, 0.3333],\n",
       "           [0.7765, 0.7765, 0.7804,  ..., 0.3333, 0.3294, 0.3294],\n",
       "           [0.7765, 0.7804, 0.7804,  ..., 0.3451, 0.3373, 0.3373],\n",
       "           ...,\n",
       "           [0.5176, 0.5020, 0.4941,  ..., 0.6078, 0.6078, 0.6039],\n",
       "           [0.5255, 0.5098, 0.4980,  ..., 0.6118, 0.6118, 0.6118],\n",
       "           [0.5333, 0.5176, 0.4980,  ..., 0.6118, 0.6157, 0.6157]],\n",
       " \n",
       "          [[0.8275, 0.8275, 0.8275,  ..., 0.6118, 0.6157, 0.6196],\n",
       "           [0.8275, 0.8275, 0.8314,  ..., 0.6275, 0.6157, 0.6157],\n",
       "           [0.8275, 0.8314, 0.8314,  ..., 0.6392, 0.6314, 0.6314],\n",
       "           ...,\n",
       "           [0.3373, 0.3216, 0.3020,  ..., 0.3843, 0.3843, 0.3804],\n",
       "           [0.3451, 0.3294, 0.3059,  ..., 0.3882, 0.3882, 0.3882],\n",
       "           [0.3529, 0.3373, 0.3059,  ..., 0.3882, 0.3922, 0.3922]],\n",
       " \n",
       "          [[0.8000, 0.8000, 0.8000,  ..., 0.6353, 0.6392, 0.6431],\n",
       "           [0.8000, 0.8000, 0.8039,  ..., 0.6471, 0.6392, 0.6392],\n",
       "           [0.8000, 0.8039, 0.8039,  ..., 0.6588, 0.6510, 0.6510],\n",
       "           ...,\n",
       "           [0.3451, 0.3294, 0.3137,  ..., 0.4078, 0.4078, 0.4039],\n",
       "           [0.3529, 0.3373, 0.3176,  ..., 0.4118, 0.4118, 0.4118],\n",
       "           [0.3608, 0.3451, 0.3176,  ..., 0.4118, 0.4157, 0.4157]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.8157, 0.8157, 0.8157,  ..., 0.0314, 0.0275, 0.0275],\n",
       "           [0.8157, 0.8157, 0.8196,  ..., 0.1098, 0.1098, 0.1098],\n",
       "           [0.8157, 0.8196, 0.8196,  ..., 0.2549, 0.2588, 0.2588],\n",
       "           ...,\n",
       "           [0.7725, 0.7882, 0.6157,  ..., 0.6902, 0.6941, 0.6941],\n",
       "           [0.7647, 0.7529, 0.5529,  ..., 0.6902, 0.6941, 0.6941],\n",
       "           [0.7529, 0.7294, 0.5098,  ..., 0.6902, 0.6941, 0.6941]],\n",
       " \n",
       "          [[0.8706, 0.8706, 0.8706,  ..., 0.1608, 0.1569, 0.1569],\n",
       "           [0.8706, 0.8706, 0.8745,  ..., 0.2549, 0.2549, 0.2549],\n",
       "           [0.8706, 0.8745, 0.8745,  ..., 0.4353, 0.4392, 0.4392],\n",
       "           ...,\n",
       "           [0.7922, 0.8078, 0.6353,  ..., 0.1255, 0.1294, 0.1294],\n",
       "           [0.7843, 0.7725, 0.5725,  ..., 0.1255, 0.1294, 0.1294],\n",
       "           [0.7725, 0.7490, 0.5294,  ..., 0.1255, 0.1294, 0.1294]],\n",
       " \n",
       "          [[0.8706, 0.8706, 0.8706,  ..., 0.1882, 0.1843, 0.1843],\n",
       "           [0.8706, 0.8706, 0.8745,  ..., 0.2902, 0.2902, 0.2902],\n",
       "           [0.8706, 0.8745, 0.8745,  ..., 0.4941, 0.4980, 0.4980],\n",
       "           ...,\n",
       "           [0.8039, 0.8196, 0.6471,  ..., 0.1647, 0.1686, 0.1686],\n",
       "           [0.7961, 0.7843, 0.5843,  ..., 0.1647, 0.1686, 0.1686],\n",
       "           [0.7843, 0.7608, 0.5412,  ..., 0.1647, 0.1686, 0.1686]]],\n",
       " \n",
       " \n",
       "         [[[0.5686, 0.5686, 0.5686,  ..., 0.7255, 0.7255, 0.7255],\n",
       "           [0.5686, 0.5686, 0.5647,  ..., 0.7255, 0.7255, 0.7255],\n",
       "           [0.5647, 0.5647, 0.5647,  ..., 0.7255, 0.7255, 0.7255],\n",
       "           ...,\n",
       "           [0.3412, 0.3490, 0.3569,  ..., 0.3804, 0.3882, 0.3961],\n",
       "           [0.3373, 0.3451, 0.3569,  ..., 0.4118, 0.4235, 0.4275],\n",
       "           [0.3333, 0.3412, 0.3529,  ..., 0.4392, 0.4510, 0.4471]],\n",
       " \n",
       "          [[0.5804, 0.5804, 0.5804,  ..., 0.7490, 0.7490, 0.7490],\n",
       "           [0.5804, 0.5804, 0.5765,  ..., 0.7490, 0.7490, 0.7490],\n",
       "           [0.5765, 0.5765, 0.5765,  ..., 0.7490, 0.7490, 0.7490],\n",
       "           ...,\n",
       "           [0.2431, 0.2510, 0.2588,  ..., 0.3882, 0.3961, 0.4039],\n",
       "           [0.2392, 0.2471, 0.2588,  ..., 0.4196, 0.4314, 0.4353],\n",
       "           [0.2353, 0.2431, 0.2549,  ..., 0.4471, 0.4588, 0.4549]],\n",
       " \n",
       "          [[0.5529, 0.5529, 0.5529,  ..., 0.7333, 0.7333, 0.7333],\n",
       "           [0.5529, 0.5529, 0.5490,  ..., 0.7333, 0.7333, 0.7333],\n",
       "           [0.5490, 0.5490, 0.5490,  ..., 0.7333, 0.7333, 0.7333],\n",
       "           ...,\n",
       "           [0.1216, 0.1294, 0.1373,  ..., 0.3686, 0.3765, 0.3843],\n",
       "           [0.1176, 0.1255, 0.1373,  ..., 0.4000, 0.4118, 0.4157],\n",
       "           [0.1137, 0.1216, 0.1333,  ..., 0.4275, 0.4392, 0.4353]]],\n",
       " \n",
       " \n",
       "         [[[0.7882, 0.7882, 0.7882,  ..., 0.5725, 0.5725, 0.5725],\n",
       "           [0.7882, 0.7882, 0.7882,  ..., 0.5725, 0.5725, 0.5725],\n",
       "           [0.7882, 0.7882, 0.7882,  ..., 0.5725, 0.5725, 0.5725],\n",
       "           ...,\n",
       "           [0.1412, 0.1412, 0.1373,  ..., 0.1098, 0.1216, 0.1294],\n",
       "           [0.1412, 0.1412, 0.1373,  ..., 0.1098, 0.1176, 0.1255],\n",
       "           [0.1412, 0.1412, 0.1373,  ..., 0.1059, 0.1137, 0.1216]],\n",
       " \n",
       "          [[0.7529, 0.7529, 0.7529,  ..., 0.2196, 0.2196, 0.2196],\n",
       "           [0.7529, 0.7529, 0.7529,  ..., 0.2196, 0.2196, 0.2196],\n",
       "           [0.7529, 0.7529, 0.7529,  ..., 0.2196, 0.2196, 0.2196],\n",
       "           ...,\n",
       "           [0.1451, 0.1451, 0.1412,  ..., 0.1176, 0.1294, 0.1373],\n",
       "           [0.1451, 0.1451, 0.1412,  ..., 0.1176, 0.1255, 0.1333],\n",
       "           [0.1451, 0.1451, 0.1412,  ..., 0.1137, 0.1216, 0.1294]],\n",
       " \n",
       "          [[0.7176, 0.7176, 0.7176,  ..., 0.2588, 0.2588, 0.2588],\n",
       "           [0.7176, 0.7176, 0.7176,  ..., 0.2588, 0.2588, 0.2588],\n",
       "           [0.7176, 0.7176, 0.7176,  ..., 0.2588, 0.2588, 0.2588],\n",
       "           ...,\n",
       "           [0.1529, 0.1529, 0.1490,  ..., 0.1137, 0.1255, 0.1333],\n",
       "           [0.1529, 0.1529, 0.1490,  ..., 0.1137, 0.1216, 0.1294],\n",
       "           [0.1529, 0.1529, 0.1490,  ..., 0.1098, 0.1176, 0.1255]]]]),\n",
       " tensor([ 5,  4,  4,  7,  1,  0, 15, 16, 10,  0,  3, 16, 16,  0,  0, 12,  3, 16,\n",
       "          1, 15,  0,  3,  0,  3,  3,  9,  7, 10,  4,  4,  3,  4,  4,  3,  4,  4,\n",
       "          1,  0,  1,  4,  0,  3, 16,  3,  3,  5, 12,  4,  9, 12, 16,  4,  1,  2,\n",
       "         16,  4,  3,  1,  4,  1, 12,  4,  3,  1])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de79139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13743c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "model_resnet = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_resnet.fc.in_features\n",
    "model_resnet.fc = nn.Linear(num_ftrs, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af859eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cb31980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워크 출력 채널 개수 (예측 class type 개수) 18\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "torch.nn.init.xavier_uniform_(model_resnet.fc.weight)\n",
    "stdv = 1. / math.sqrt(model_resnet.fc.weight.size(1))\n",
    "model_resnet.fc.bias.data.uniform_(-stdv, stdv)\n",
    "print(\"네트워크 출력 채널 개수 (예측 class type 개수)\", model_resnet.fc.weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9ba3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.to(device) # Resnent 18 네트워크의 Tensor들을 GPU에 올릴지 Memory에 올릴지 결정함\n",
    "\n",
    "LEARNING_RATE = 0.0001 # 학습 때 사용하는 optimizer의 학습률 옵션 설정\n",
    "NUM_EPOCH = 5 # 학습 때 mnist train 데이터 셋을 얼마나 많이 학습할지 결정하는 옵션\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # 분류 학습 때 많이 사용되는 Cross entropy loss를 objective function으로 사용 - https://en.wikipedia.org/wiki/Cross_entropy\n",
    "optimizer = torch.optim.Adam(model_resnet.parameters(), lr=LEARNING_RATE) # weight 업데이트를 위한 optimizer를 Adam으로 사용함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b420823f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Epoch: 1 [0/18899 (0%)]\tLoss: 0.000559\n",
      "Train_Epoch: 1 [6400/18899 (34%)]\tLoss: 0.000967\n",
      "Train_Epoch: 1 [12800/18899 (68%)]\tLoss: 0.002054\n",
      "Train_Epoch: 2 [0/18899 (0%)]\tLoss: 0.000696\n",
      "Train_Epoch: 2 [6400/18899 (34%)]\tLoss: 0.033600\n",
      "Train_Epoch: 2 [12800/18899 (68%)]\tLoss: 0.008991\n",
      "Train_Epoch: 3 [0/18899 (0%)]\tLoss: 0.053249\n",
      "Train_Epoch: 3 [6400/18899 (34%)]\tLoss: 0.010896\n",
      "Train_Epoch: 3 [12800/18899 (68%)]\tLoss: 0.010211\n",
      "Train_Epoch: 4 [0/18899 (0%)]\tLoss: 0.000609\n",
      "Train_Epoch: 4 [6400/18899 (34%)]\tLoss: 0.000564\n",
      "Train_Epoch: 4 [12800/18899 (68%)]\tLoss: 0.000174\n",
      "Train_Epoch: 5 [0/18899 (0%)]\tLoss: 0.002156\n",
      "Train_Epoch: 5 [6400/18899 (34%)]\tLoss: 0.001213\n",
      "Train_Epoch: 5 [12800/18899 (68%)]\tLoss: 0.001619\n",
      "Train_Epoch: 6 [0/18899 (0%)]\tLoss: 0.000361\n",
      "Train_Epoch: 6 [6400/18899 (34%)]\tLoss: 0.000457\n",
      "Train_Epoch: 6 [12800/18899 (68%)]\tLoss: 0.000452\n",
      "Train_Epoch: 7 [0/18899 (0%)]\tLoss: 0.000195\n",
      "Train_Epoch: 7 [6400/18899 (34%)]\tLoss: 0.000177\n",
      "Train_Epoch: 7 [12800/18899 (68%)]\tLoss: 0.000046\n",
      "Train_Epoch: 8 [0/18899 (0%)]\tLoss: 0.000032\n",
      "Train_Epoch: 8 [6400/18899 (34%)]\tLoss: 0.000068\n",
      "Train_Epoch: 8 [12800/18899 (68%)]\tLoss: 0.000083\n",
      "Train_Epoch: 9 [0/18899 (0%)]\tLoss: 0.000153\n",
      "Train_Epoch: 9 [6400/18899 (34%)]\tLoss: 0.000107\n",
      "Train_Epoch: 9 [12800/18899 (68%)]\tLoss: 0.000213\n",
      "Train_Epoch: 10 [0/18899 (0%)]\tLoss: 0.000180\n",
      "Train_Epoch: 10 [6400/18899 (34%)]\tLoss: 0.000120\n",
      "Train_Epoch: 10 [12800/18899 (68%)]\tLoss: 0.000083\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCH = 10\n",
    "log_interval = 100 # 로그를 보기위한 간격\n",
    "\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, NUM_EPOCH + 1):\n",
    "  # Train Mode\n",
    "  model_resnet.train()\n",
    "    \n",
    "  for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model_resnet(data)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train_Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "      \tepoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "      \t100 * batch_idx / len(train_dataloader), loss.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d5aed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81733001",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval'\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "test_dataset = TestDataset(image_paths, transform=transforms.ToTensor())\n",
    "\n",
    "test_loader = DataLoader(test_dataset,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c932e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "model_resnet.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model_resnet(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
