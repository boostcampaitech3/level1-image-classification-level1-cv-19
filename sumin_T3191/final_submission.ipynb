{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sound-volume",
   "metadata": {
    "id": "sound-volume"
   },
   "source": [
    "# Final Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-appeal",
   "metadata": {
    "id": "gentle-appeal"
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "closed-launch",
   "metadata": {
    "id": "closed-launch",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "filled-configuration",
   "metadata": {
    "id": "filled-configuration",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=3, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informative-banks",
   "metadata": {
    "id": "informative-banks",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\n",
    "class F1Loss(nn.Module):\n",
    "    def __init__(self, classes=3, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.ndim == 2\n",
    "        assert y_true.ndim == 1\n",
    "        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\n",
    "        return 1 - f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "desirable-accounting",
   "metadata": {
    "id": "desirable-accounting",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_criterion_entrypoints = {\n",
    "    'cross_entropy': nn.CrossEntropyLoss,\n",
    "    'focal': FocalLoss,\n",
    "    'label_smoothing': LabelSmoothingLoss,\n",
    "    'f1': F1Loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-picking",
   "metadata": {
    "id": "gothic-picking"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c1313c",
   "metadata": {
    "id": "b9c1313c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8b3f99",
   "metadata": {
    "id": "0f8b3f99"
   },
   "outputs": [],
   "source": [
    "# # -- Hyperparameter 정의.\n",
    "# LR = 0.01  #Learning rate\n",
    "# BATCH_SIZE = 32 \n",
    "# EPOCH = 15 \n",
    "\n",
    "# x = torch.unsqueeze(torch.linspace(-1,1,1000),dim=1)\n",
    "# y = x.pow(2) + 0.1*torch.normal(torch.zeros(x.size()))\n",
    "\n",
    "# # -- 데이터셋 생성.\n",
    "# torch_dataset = Data.TensorDataset(x,y)\n",
    "# loader = Data.DataLoader(\n",
    "#     dataset=torch_dataset,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "#     num_workers=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d863901",
   "metadata": {
    "id": "1d863901"
   },
   "outputs": [],
   "source": [
    "# # -- 각 optimizer를 사용한 신경망 정의.\n",
    "# net_SGD = Net()\n",
    "# net_Adam = Net()\n",
    "\n",
    "# # -- 각 신경망 리스트.\n",
    "# nets = [net_SGD, net_Adam]\n",
    "\n",
    "# # -- 각 Optimizer 정의.\n",
    "# opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR) # SGD Optimizer\n",
    "# opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9,0.99)) # Adam Optimizer\n",
    "\n",
    "# # -- 각 Optimizer 리스트.\n",
    "# optimizers = [opt_SGD, opt_Adam]\n",
    "\n",
    "# # -- Loss 계산.\n",
    "# loss_func = torch.nn.MSELoss()\n",
    "# # -- Loss 기록.\n",
    "# losses_his = [[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec5887c5",
   "metadata": {
    "id": "ec5887c5"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset import MaskBaseDataset\n",
    "from loss import create_criterion\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d2986",
   "metadata": {
    "id": "340d2986"
   },
   "source": [
    "###### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4878c25",
   "metadata": {
    "id": "a4878c25"
   },
   "outputs": [],
   "source": [
    "# -- loss & metric\n",
    "# criterion = create_criterion(args.criterion)  # default: cross_entropy\n",
    "criterion = create_criterion('label_smoothing')  # default: cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8705f8-7eec-42b0-8f16-c66245ffc35a",
   "metadata": {},
   "source": [
    "## Model\n",
    "- ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5503ca3d-6e9e-4671-b92f-edf515ebd607",
   "metadata": {
    "id": "tsZbkfObUSg5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /opt/ml/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0dc6e107e6478889fe0620e84890a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "model = resnet50(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be6549-31d6-4ebc-b612-77c5b16071b0",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e6b10b4-b6fc-4810-bb52-088a33c17971",
   "metadata": {
    "id": "xQIxW_QRUSg6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=18, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 18\n",
    "# model = resnet152(pretrained=True)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(1000, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, num_classes),\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb31fd7-59cb-4165-95e4-676a15eea0d2",
   "metadata": {
    "id": "LM4aPLqeUSg8"
   },
   "source": [
    "### Weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3623a67-7993-4603-81cb-4ee98d475b26",
   "metadata": {
    "id": "fbqr7onnUSg_"
   },
   "source": [
    "#### task specific 한 부분만 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dff008bc-eaca-4e31-a202-ec1c2a009f8a",
   "metadata": {
    "id": "I_5s0otKUSg8"
   },
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def initialize_weights(model):\n",
    "    \"\"\"\n",
    "    Xavier uniform 분포로 모든 weight 를 초기화합니다.\n",
    "    더 많은 weight 초기화 방법은 다음 문서에서 참고해주세요. https://pytorch.org/docs/stable/nn.init.html\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.01)\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbeaaeec-2dd6-4e44-ad94-fa3c792314ba",
   "metadata": {
    "id": "kvlDANr6USg_"
   },
   "outputs": [],
   "source": [
    "# Classifier 부분만 initialize\n",
    "initialize_weights(model.classifier)\n",
    "\n",
    "# Weight Initialization 이후 모델 feature 파트의 첫번째 weight 분포\n",
    "# classifier 부분만 xavier uniform 으로 초기화해서 feature 파트는 uniform 한 분포를 가지지 않는 것을 확인할 수 있습니다.\n",
    "# plt.hist(model.features[0].weight.detach().numpy().reshape(-1)) \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008b57e",
   "metadata": {
    "id": "e008b57e"
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "200c7376-64da-485e-85dc-3c9be250817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d21259b",
   "metadata": {
    "id": "9d21259b"
   },
   "outputs": [],
   "source": [
    "opt_module = getattr(import_module(\"torch.optim\"), 'Adam')\n",
    "optimizer = opt_module(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=0.0001,\n",
    "    weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-merchandise",
   "metadata": {
    "id": "informative-merchandise"
   },
   "source": [
    "### Scheduler\n",
    "- CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e218df88-f83b-4fd8-853c-c38e915fb61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "union-laundry",
   "metadata": {
    "id": "union-laundry"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Accuracy\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd6e9c46-a1e3-43b8-b2db-867ddc0d85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad794f20-c91e-4fb3-a72f-e8d3151f5d9a",
   "metadata": {
    "id": "complex-israel"
   },
   "outputs": [],
   "source": [
    "### Configurations\n",
    "# data_dir = '학습 이미지 폴더의 경로를 입력해주세요.'\n",
    "data_dir = '../../ml/input/data/train'\n",
    "img_dir = f'{data_dir}/images'\n",
    "df_path = f'{data_dir}/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e729faa-9b28-45fa-ab78-8b3b379b7114",
   "metadata": {
    "id": "whole-computer",
    "outputId": "4d9f2e50-2b17-4ade-837a-e6edf47adf4b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  gender   race  age                    path\n",
       "0  000001  female  Asian   45  000001_female_Asian_45\n",
       "1  000002  female  Asian   52  000002_female_Asian_52\n",
       "2  000004    male  Asian   54    000004_male_Asian_54\n",
       "3  000005  female  Asian   58  000005_female_Asian_58\n",
       "4  000006  female  Asian   59  000006_female_Asian_59"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(df_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077f77c-a11d-46e0-8e70-2a662f50ae79",
   "metadata": {
    "id": "minute-neighborhood"
   },
   "source": [
    "## 1. Image Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81bcde51-94fa-42be-b67b-125b1148126f",
   "metadata": {
    "id": "crude-frank"
   },
   "outputs": [],
   "source": [
    "def get_ext(img_dir, img_id):\n",
    "    filename = os.listdir(os.path.join(img_dir, img_id))[0]\n",
    "    ext = os.path.splitext(filename)[-1].lower()\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32ef46-2030-4339-899f-5194998e8136",
   "metadata": {
    "id": "compliant-operation"
   },
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95117090-5dd5-4614-bf86-83b62a7a432b",
   "metadata": {
    "id": "democratic-juvenile"
   },
   "source": [
    "## 2.1 Augmentation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7dce66c-e5db-438d-a62f-fef75b5e1a1b",
   "metadata": {
    "id": "horizontal-strain"
   },
   "outputs": [],
   "source": [
    "# mean, std = (0.5, 0.5, 0.5), (0.2, 0.2, 0.2)\n",
    "mean, std = (0.56019358, 0.52410121, 0.501457), (0.23318603, 0.24300033, 0.24567522)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baffddc6-832b-467e-8ef2-af24bf04a9b6",
   "metadata": {
    "id": "sudden-survivor"
   },
   "source": [
    "### 2.1.2 Albumentation Style Augmentation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a2c3bf2-8c7a-45bc-89ef-07f3d1d5852d",
   "metadata": {
    "id": "national-diameter"
   },
   "outputs": [],
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    \"\"\"\n",
    "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "    Args:\n",
    "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "    Returns:\n",
    "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            GaussNoise(p=0.5),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            Resize(img_size[0], img_size[1]),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f62b0-598e-4e21-9518-1519d5796877",
   "metadata": {
    "id": "partial-lafayette"
   },
   "source": [
    "### 2.2 Define Dataset\n",
    "\n",
    "- 이미지와 레이블을 출력하는 Dataset 클래스를 정의\n",
    "- 레이블은 마스크 여부, 성별, 나이로 결정\n",
    "- 레이블은 3(마스크 착용, 미착용, 잘못착용) * 2(남성, 여성) * 3(30세 미만, 30세-60세, 60세 이상) 으로 총 18개가 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e24ec9da-fe6f-4249-8406-6fba717fd5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65b51f6f-f59a-48d2-ad77-9463e9981c7d",
   "metadata": {
    "id": "annoying-emission"
   },
   "outputs": [],
   "source": [
    "### 마스크 여부, 성별, 나이를 mapping할 클래스를 생성합니다.\n",
    "\n",
    "class MaskLabels(int, Enum):\n",
    "    MASK = 0\n",
    "    INCORRECT = 1\n",
    "    NORMAL = 2\n",
    "\n",
    "\n",
    "class GenderLabels(int, Enum):\n",
    "    MALE = 0\n",
    "    FEMALE = 1\n",
    "\n",
    "    @classmethod\n",
    "    def from_str(cls, value: str) -> int:\n",
    "        value = value.lower()\n",
    "        if value == \"male\":\n",
    "            return cls.MALE\n",
    "        elif value == \"female\":\n",
    "            return cls.FEMALE\n",
    "        else:\n",
    "            raise ValueError(f\"Gender value should be either 'male' or 'female', {value}\")\n",
    "\n",
    "\n",
    "class AgeLabels(int, Enum):\n",
    "    YOUNG = 0\n",
    "    MIDDLE = 1\n",
    "    OLD = 2\n",
    "\n",
    "    @classmethod\n",
    "    def from_number(cls, value: str) -> int:\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Age value should be numeric, {value}\")\n",
    "\n",
    "        if value < 30:\n",
    "            return cls.YOUNG\n",
    "        elif value < 60:\n",
    "            return cls.MIDDLE\n",
    "        else:\n",
    "            return cls.OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ec10680-9151-47b7-a57e-feae4c504363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "457314ce-8774-4ff1-84cf-8cc1b996cc6b",
   "metadata": {
    "id": "divine-transmission"
   },
   "outputs": [],
   "source": [
    "class MaskBaseDataset(data.Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1\": MaskLabels.MASK,\n",
    "        \"mask2\": MaskLabels.MASK,\n",
    "        \"mask3\": MaskLabels.MASK,\n",
    "        \"mask4\": MaskLabels.MASK,\n",
    "        \"mask5\": MaskLabels.MASK,\n",
    "        \"incorrect_mask\": MaskLabels.INCORRECT,\n",
    "        \"normal\": MaskLabels.NORMAL\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, img_dir, mean, std, transform=None):\n",
    "        \"\"\"\n",
    "        MaskBaseDataset을 initialize 합니다.\n",
    "\n",
    "        Args:\n",
    "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
    "            transform: Augmentation을 하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transform\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
    "        \"\"\"\n",
    "        profiles = os.listdir(self.img_dir)\n",
    "        for profile in profiles:\n",
    "            if profile.startswith(\".\"):  # \".\" 로 시작하는 파일은 무시합니다\n",
    "                continue\n",
    "\n",
    "            img_folder = os.path.join(self.img_dir, profile)\n",
    "            for file_name in os.listdir(img_folder):\n",
    "                _file_name, ext = os.path.splitext(file_name)\n",
    "                if _file_name not in self._file_names:  # \".\" 로 시작하는 파일 및 invalid 한 파일들은 무시합니다\n",
    "                    continue\n",
    "\n",
    "                img_path = os.path.join(self.img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                mask_label = self._file_names[_file_name]\n",
    "\n",
    "                id, gender, race, age = profile.split(\"_\")\n",
    "                gender_label = GenderLabels.from_str(gender)\n",
    "                age_label = AgeLabels.from_number(age)\n",
    "\n",
    "                self.image_paths.append(img_path)\n",
    "                self.mask_labels.append(mask_label)\n",
    "                self.gender_labels.append(gender_label)\n",
    "                self.age_labels.append(age_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 레이블을 불러옵니다.\n",
    "        mask_label = self.mask_labels[index]\n",
    "        gender_label = self.gender_labels[index]\n",
    "        age_label = self.age_labels[index]\n",
    "        multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image=np.array(image))['image']\n",
    "        return image_transform, multi_class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9defecd-9dba-461f-a44f-0b1fd0dc6f6e",
   "metadata": {
    "id": "secure-plasma"
   },
   "outputs": [],
   "source": [
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "dataset = MaskBaseDataset(\n",
    "    img_dir=img_dir,\n",
    "    mean=mean,\n",
    "    std=std\n",
    ")\n",
    "\n",
    "# train dataset과 validation dataset을 8:2 비율로 나눕니다.\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_train = len(dataset) - n_val\n",
    "train_dataset, val_dataset = data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# 각 dataset에 augmentation 함수를 설정합니다.\n",
    "train_dataset.dataset.set_transform(transform['train'])\n",
    "val_dataset.dataset.set_transform(transform['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd41b7-14a4-4c25-bde3-d6de6544f71f",
   "metadata": {
    "id": "regulation-membrane"
   },
   "source": [
    "## 3. DataLoader\n",
    "- 정의한 Dataset을 바탕으로 DataLoader을 생성합니다.\n",
    "- Dataset은 이미지 한장을 주는 모듈이라면, DataLoader은 여러 이미지를 batch_size만큼 묶어 전달해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c084069-89db-40d2-9c28-c690ae7eb749",
   "metadata": {
    "id": "blessed-robert"
   },
   "outputs": [],
   "source": [
    "# training dataloader은 데이터를 섞어주어야 합니다. (shuffle=True)\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-guess",
   "metadata": {
    "id": "foreign-guess"
   },
   "source": [
    "## 5. Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-moisture",
   "metadata": {
    "id": "bulgarian-moisture"
   },
   "source": [
    "### 5.1 Callback - Checkpoint, Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "838cb22d-c695-4bba-98ec-1ff76a251bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b536e520-e41f-4634-8e1c-556e5bd5b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "insured-craft",
   "metadata": {
    "id": "insured-craft"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=18, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Callback1: Checkpoint - Accuracy가 높아질 때마다 모델을 저장합니다.\n",
    "# 학습 코드에서 이어집니다.\n",
    "\n",
    "# -- Callback2: Early Stopping - 성능이 일정 기간동안 향상이 없을 경우 학습을 종료합니다.\n",
    "patience = 10\n",
    "counter = 0\n",
    "num_epochs = 40\n",
    "name = 'final'\n",
    "model.to(device)\n",
    "# 학습 코드에서 이어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-citizenship",
   "metadata": {
    "id": "turned-citizenship"
   },
   "source": [
    "### 5.2 Training Method - Gradient Accumulation\n",
    "- Graident Accumulation은 한 iteration에 파라미터를 업데이트시키는게 아니라, gradient를 여러 iteration 동안 쌓아서 업데이트시킵니다. 한 번에 파라미터를 업데이트시키는 건 noise가 있을 수 있으므로, 여러번 쌓아서 한번에 업데이트 시킴으로써 그러한 문제를 방지하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bibliographic-recycling",
   "metadata": {
    "id": "bibliographic-recycling"
   },
   "outputs": [],
   "source": [
    "# -- Gradient Accumulation\n",
    "accumulation_steps = 2\n",
    "# 학습코드에서 이어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-poker",
   "metadata": {
    "id": "premium-poker"
   },
   "source": [
    "### 5.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24b612c2",
   "metadata": {
    "id": "24b612c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/40](100/945) || training loss 0.07737 || training accuracy 98.03% || lr [5e-05]\n",
      "Epoch[0/40](200/945) || training loss 0.05372 || training accuracy 98.66% || lr [5e-05]\n",
      "Epoch[0/40](300/945) || training loss 0.05349 || training accuracy 98.59% || lr [5e-05]\n",
      "Epoch[0/40](400/945) || training loss 0.05715 || training accuracy 98.44% || lr [5e-05]\n",
      "Epoch[0/40](500/945) || training loss 0.03607 || training accuracy 99.28% || lr [5e-05]\n",
      "Epoch[0/40](600/945) || training loss 0.03427 || training accuracy 99.12% || lr [5e-05]\n",
      "Epoch[0/40](700/945) || training loss 0.03224 || training accuracy 99.28% || lr [5e-05]\n",
      "Epoch[0/40](800/945) || training loss 0.02736 || training accuracy 99.59% || lr [5e-05]\n",
      "Epoch[0/40](900/945) || training loss 0.02277 || training accuracy 99.62% || lr [5e-05]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 99.72%, loss: 0.011 || best acc : 99.72%, best loss: 0.011\n",
      "Epoch[1/40](100/945) || training loss 0.01258 || training accuracy 99.88% || lr [0.0]\n",
      "Epoch[1/40](200/945) || training loss 0.01527 || training accuracy 99.69% || lr [0.0]\n",
      "Epoch[1/40](300/945) || training loss 0.01229 || training accuracy 99.94% || lr [0.0]\n",
      "Epoch[1/40](400/945) || training loss 0.01455 || training accuracy 99.78% || lr [0.0]\n",
      "Epoch[1/40](500/945) || training loss 0.01306 || training accuracy 99.88% || lr [0.0]\n",
      "Epoch[1/40](600/945) || training loss 0.0126 || training accuracy 99.84% || lr [0.0]\n",
      "Epoch[1/40](700/945) || training loss 0.01235 || training accuracy 99.88% || lr [0.0]\n",
      "Epoch[1/40](800/945) || training loss 0.01259 || training accuracy 99.81% || lr [0.0]\n",
      "Epoch[1/40](900/945) || training loss 0.0142 || training accuracy 99.78% || lr [0.0]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 99.79%, loss: 0.01 || best acc : 99.79%, best loss: 0.01\n",
      "Epoch[2/40](100/945) || training loss 0.02255 || training accuracy 99.44% || lr [4.9999999999999996e-05]\n",
      "Epoch[2/40](200/945) || training loss 0.02618 || training accuracy 99.44% || lr [4.9999999999999996e-05]\n",
      "Epoch[2/40](300/945) || training loss 0.03236 || training accuracy 99.19% || lr [4.9999999999999996e-05]\n",
      "Epoch[2/40](400/945) || training loss 0.02594 || training accuracy 99.34% || lr [4.9999999999999996e-05]\n",
      "Epoch[2/40](500/945) || training loss 0.02256 || training accuracy 99.56% || lr [4.9999999999999996e-05]\n",
      "Epoch[2/40](600/945) || training loss 0.01842 || training accuracy 99.62% || lr [4.9999999999999996e-05]\n",
      "Epoch[2/40](700/945) || training loss 0.02186 || training accuracy 99.41% || lr [4.9999999999999996e-05]\n",
      "Epoch[2/40](800/945) || training loss 0.03039 || training accuracy 98.97% || lr [4.9999999999999996e-05]\n",
      "Epoch[2/40](900/945) || training loss 0.02118 || training accuracy 99.56% || lr [4.9999999999999996e-05]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.79%, loss: 0.01 || best acc : 99.79%, best loss: 0.01\n",
      "Epoch[3/40](100/945) || training loss 0.03013 || training accuracy 99.09% || lr [0.00010000000000000002]\n",
      "Epoch[3/40](200/945) || training loss 0.1037 || training accuracy 96.94% || lr [0.00010000000000000002]\n",
      "Epoch[3/40](300/945) || training loss 0.1126 || training accuracy 96.38% || lr [0.00010000000000000002]\n",
      "Epoch[3/40](400/945) || training loss 0.1195 || training accuracy 96.25% || lr [0.00010000000000000002]\n",
      "Epoch[3/40](500/945) || training loss 0.06632 || training accuracy 97.78% || lr [0.00010000000000000002]\n",
      "Epoch[3/40](600/945) || training loss 0.04751 || training accuracy 98.56% || lr [0.00010000000000000002]\n",
      "Epoch[3/40](700/945) || training loss 0.04436 || training accuracy 98.81% || lr [0.00010000000000000002]\n",
      "Epoch[3/40](800/945) || training loss 0.03266 || training accuracy 98.97% || lr [0.00010000000000000002]\n",
      "Epoch[3/40](900/945) || training loss 0.03732 || training accuracy 98.81% || lr [0.00010000000000000002]\n",
      "Calculating validation results...\n",
      "[Val] acc : 98.68%, loss: 0.046 || best acc : 99.79%, best loss: 0.01\n",
      "Epoch[4/40](100/945) || training loss 0.01343 || training accuracy 99.81% || lr [5.000000000000002e-05]\n",
      "Epoch[4/40](200/945) || training loss 0.01112 || training accuracy 99.72% || lr [5.000000000000002e-05]\n",
      "Epoch[4/40](300/945) || training loss 0.009287 || training accuracy 99.75% || lr [5.000000000000002e-05]\n",
      "Epoch[4/40](400/945) || training loss 0.009985 || training accuracy 99.88% || lr [5.000000000000002e-05]\n",
      "Epoch[4/40](500/945) || training loss 0.006737 || training accuracy 99.91% || lr [5.000000000000002e-05]\n",
      "Epoch[4/40](600/945) || training loss 0.004565 || training accuracy 99.94% || lr [5.000000000000002e-05]\n",
      "Epoch[4/40](700/945) || training loss 0.00335 || training accuracy 99.97% || lr [5.000000000000002e-05]\n",
      "Epoch[4/40](800/945) || training loss 0.003503 || training accuracy 99.94% || lr [5.000000000000002e-05]\n",
      "Epoch[4/40](900/945) || training loss 0.003479 || training accuracy 99.94% || lr [5.000000000000002e-05]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 99.92%, loss: 0.0068 || best acc : 99.92%, best loss: 0.0068\n",
      "Epoch[5/40](100/945) || training loss 0.003388 || training accuracy 99.91% || lr [0.0]\n",
      "Epoch[5/40](200/945) || training loss 0.003289 || training accuracy 99.97% || lr [0.0]\n",
      "Epoch[5/40](300/945) || training loss 0.004328 || training accuracy 99.94% || lr [0.0]\n",
      "Epoch[5/40](400/945) || training loss 0.002507 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[5/40](500/945) || training loss 0.002755 || training accuracy 99.97% || lr [0.0]\n",
      "Epoch[5/40](600/945) || training loss 0.002717 || training accuracy 99.94% || lr [0.0]\n",
      "Epoch[5/40](700/945) || training loss 0.003168 || training accuracy 99.97% || lr [0.0]\n",
      "Epoch[5/40](800/945) || training loss 0.003 || training accuracy 99.97% || lr [0.0]\n",
      "Epoch[5/40](900/945) || training loss 0.002152 || training accuracy 100.00% || lr [0.0]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.92%, loss: 0.0066 || best acc : 99.92%, best loss: 0.0066\n",
      "Epoch[6/40](100/945) || training loss 0.001753 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[6/40](200/945) || training loss 0.00307 || training accuracy 99.97% || lr [4.9999999999999996e-05]\n",
      "Epoch[6/40](300/945) || training loss 0.002388 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[6/40](400/945) || training loss 0.002933 || training accuracy 99.97% || lr [4.9999999999999996e-05]\n",
      "Epoch[6/40](500/945) || training loss 0.002921 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[6/40](600/945) || training loss 0.00161 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[6/40](700/945) || training loss 0.002338 || training accuracy 99.97% || lr [4.9999999999999996e-05]\n",
      "Epoch[6/40](800/945) || training loss 0.003257 || training accuracy 99.94% || lr [4.9999999999999996e-05]\n",
      "Epoch[6/40](900/945) || training loss 0.004665 || training accuracy 99.88% || lr [4.9999999999999996e-05]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.78%, loss: 0.01 || best acc : 99.92%, best loss: 0.0066\n",
      "Epoch[7/40](100/945) || training loss 0.05002 || training accuracy 98.47% || lr [0.00010000000000000003]\n",
      "Epoch[7/40](200/945) || training loss 0.199 || training accuracy 93.50% || lr [0.00010000000000000003]\n",
      "Epoch[7/40](300/945) || training loss 0.1238 || training accuracy 95.62% || lr [0.00010000000000000003]\n",
      "Epoch[7/40](400/945) || training loss 0.08406 || training accuracy 97.44% || lr [0.00010000000000000003]\n",
      "Epoch[7/40](500/945) || training loss 0.05596 || training accuracy 98.53% || lr [0.00010000000000000003]\n",
      "Epoch[7/40](600/945) || training loss 0.0368 || training accuracy 98.97% || lr [0.00010000000000000003]\n",
      "Epoch[7/40](700/945) || training loss 0.02185 || training accuracy 99.38% || lr [0.00010000000000000003]\n",
      "Epoch[7/40](800/945) || training loss 0.02047 || training accuracy 99.47% || lr [0.00010000000000000003]\n",
      "Epoch[7/40](900/945) || training loss 0.02697 || training accuracy 99.44% || lr [0.00010000000000000003]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.31%, loss: 0.022 || best acc : 99.92%, best loss: 0.0066\n",
      "Epoch[8/40](100/945) || training loss 0.01819 || training accuracy 99.47% || lr [5.0000000000000036e-05]\n",
      "Epoch[8/40](200/945) || training loss 0.007201 || training accuracy 99.94% || lr [5.0000000000000036e-05]\n",
      "Epoch[8/40](300/945) || training loss 0.005005 || training accuracy 99.94% || lr [5.0000000000000036e-05]\n",
      "Epoch[8/40](400/945) || training loss 0.005575 || training accuracy 99.88% || lr [5.0000000000000036e-05]\n",
      "Epoch[8/40](500/945) || training loss 0.005389 || training accuracy 99.91% || lr [5.0000000000000036e-05]\n",
      "Epoch[8/40](600/945) || training loss 0.004832 || training accuracy 99.94% || lr [5.0000000000000036e-05]\n",
      "Epoch[8/40](700/945) || training loss 0.002484 || training accuracy 100.00% || lr [5.0000000000000036e-05]\n",
      "Epoch[8/40](800/945) || training loss 0.00233 || training accuracy 100.00% || lr [5.0000000000000036e-05]\n",
      "Epoch[8/40](900/945) || training loss 0.002119 || training accuracy 99.97% || lr [5.0000000000000036e-05]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 99.95%, loss: 0.0062 || best acc : 99.95%, best loss: 0.0062\n",
      "Epoch[9/40](100/945) || training loss 0.001655 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[9/40](200/945) || training loss 0.001574 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[9/40](300/945) || training loss 0.001572 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[9/40](400/945) || training loss 0.001873 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[9/40](500/945) || training loss 0.00206 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[9/40](600/945) || training loss 0.002419 || training accuracy 99.94% || lr [0.0]\n",
      "Epoch[9/40](700/945) || training loss 0.002358 || training accuracy 99.94% || lr [0.0]\n",
      "Epoch[9/40](800/945) || training loss 0.001665 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[9/40](900/945) || training loss 0.002397 || training accuracy 99.97% || lr [0.0]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.95%, loss: 0.006 || best acc : 99.95%, best loss: 0.006\n",
      "Epoch[10/40](100/945) || training loss 0.001926 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[10/40](200/945) || training loss 0.002507 || training accuracy 99.94% || lr [4.9999999999999996e-05]\n",
      "Epoch[10/40](300/945) || training loss 0.001508 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[10/40](400/945) || training loss 0.001817 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[10/40](500/945) || training loss 0.001683 || training accuracy 99.97% || lr [4.9999999999999996e-05]\n",
      "Epoch[10/40](600/945) || training loss 0.002055 || training accuracy 99.94% || lr [4.9999999999999996e-05]\n",
      "Epoch[10/40](700/945) || training loss 0.0016 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[10/40](800/945) || training loss 0.00123 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[10/40](900/945) || training loss 0.002785 || training accuracy 99.94% || lr [4.9999999999999996e-05]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.88%, loss: 0.0065 || best acc : 99.95%, best loss: 0.006\n",
      "Epoch[11/40](100/945) || training loss 0.004224 || training accuracy 99.97% || lr [0.00010000000000000024]\n",
      "Epoch[11/40](200/945) || training loss 0.02478 || training accuracy 99.19% || lr [0.00010000000000000024]\n",
      "Epoch[11/40](300/945) || training loss 0.1203 || training accuracy 95.72% || lr [0.00010000000000000024]\n",
      "Epoch[11/40](400/945) || training loss 0.1574 || training accuracy 94.78% || lr [0.00010000000000000024]\n",
      "Epoch[11/40](500/945) || training loss 0.09229 || training accuracy 97.06% || lr [0.00010000000000000024]\n",
      "Epoch[11/40](600/945) || training loss 0.05286 || training accuracy 98.41% || lr [0.00010000000000000024]\n",
      "Epoch[11/40](700/945) || training loss 0.05656 || training accuracy 98.31% || lr [0.00010000000000000024]\n",
      "Epoch[11/40](800/945) || training loss 0.04917 || training accuracy 98.72% || lr [0.00010000000000000024]\n",
      "Epoch[11/40](900/945) || training loss 0.03127 || training accuracy 99.31% || lr [0.00010000000000000024]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.68%, loss: 0.019 || best acc : 99.95%, best loss: 0.006\n",
      "Epoch[12/40](100/945) || training loss 0.01178 || training accuracy 99.75% || lr [5.000000000000007e-05]\n",
      "Epoch[12/40](200/945) || training loss 0.007796 || training accuracy 99.94% || lr [5.000000000000007e-05]\n",
      "Epoch[12/40](300/945) || training loss 0.006338 || training accuracy 99.84% || lr [5.000000000000007e-05]\n",
      "Epoch[12/40](400/945) || training loss 0.005663 || training accuracy 99.91% || lr [5.000000000000007e-05]\n",
      "Epoch[12/40](500/945) || training loss 0.003578 || training accuracy 99.94% || lr [5.000000000000007e-05]\n",
      "Epoch[12/40](600/945) || training loss 0.002853 || training accuracy 100.00% || lr [5.000000000000007e-05]\n",
      "Epoch[12/40](700/945) || training loss 0.00278 || training accuracy 100.00% || lr [5.000000000000007e-05]\n",
      "Epoch[12/40](800/945) || training loss 0.003401 || training accuracy 99.97% || lr [5.000000000000007e-05]\n",
      "Epoch[12/40](900/945) || training loss 0.001765 || training accuracy 100.00% || lr [5.000000000000007e-05]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.95%, loss: 0.0051 || best acc : 99.95%, best loss: 0.0051\n",
      "Epoch[13/40](100/945) || training loss 0.001461 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[13/40](200/945) || training loss 0.001815 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[13/40](300/945) || training loss 0.001241 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[13/40](400/945) || training loss 0.0023 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[13/40](500/945) || training loss 0.002082 || training accuracy 99.94% || lr [0.0]\n",
      "Epoch[13/40](600/945) || training loss 0.002387 || training accuracy 99.97% || lr [0.0]\n",
      "Epoch[13/40](700/945) || training loss 0.001827 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[13/40](800/945) || training loss 0.001622 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[13/40](900/945) || training loss 0.001363 || training accuracy 100.00% || lr [0.0]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.95%, loss: 0.0053 || best acc : 99.95%, best loss: 0.0051\n",
      "Epoch[14/40](100/945) || training loss 0.00216 || training accuracy 99.97% || lr [4.9999999999999996e-05]\n",
      "Epoch[14/40](200/945) || training loss 0.002436 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[14/40](300/945) || training loss 0.002226 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[14/40](400/945) || training loss 0.001932 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[14/40](500/945) || training loss 0.001561 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[14/40](600/945) || training loss 0.001957 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[14/40](700/945) || training loss 0.00249 || training accuracy 99.97% || lr [4.9999999999999996e-05]\n",
      "Epoch[14/40](800/945) || training loss 0.004149 || training accuracy 99.84% || lr [4.9999999999999996e-05]\n",
      "Epoch[14/40](900/945) || training loss 0.006653 || training accuracy 99.88% || lr [4.9999999999999996e-05]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.91%, loss: 0.0058 || best acc : 99.95%, best loss: 0.0051\n",
      "Epoch[15/40](100/945) || training loss 0.02066 || training accuracy 99.34% || lr [0.00010000000000000026]\n",
      "Epoch[15/40](200/945) || training loss 0.07037 || training accuracy 98.12% || lr [0.00010000000000000026]\n",
      "Epoch[15/40](300/945) || training loss 0.1133 || training accuracy 96.09% || lr [0.00010000000000000026]\n",
      "Epoch[15/40](400/945) || training loss 0.06603 || training accuracy 98.12% || lr [0.00010000000000000026]\n",
      "Epoch[15/40](500/945) || training loss 0.05284 || training accuracy 98.38% || lr [0.00010000000000000026]\n",
      "Epoch[15/40](600/945) || training loss 0.05819 || training accuracy 98.44% || lr [0.00010000000000000026]\n",
      "Epoch[15/40](700/945) || training loss 0.0457 || training accuracy 98.47% || lr [0.00010000000000000026]\n",
      "Epoch[15/40](800/945) || training loss 0.03257 || training accuracy 98.94% || lr [0.00010000000000000026]\n",
      "Epoch[15/40](900/945) || training loss 0.02662 || training accuracy 99.34% || lr [0.00010000000000000026]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.31%, loss: 0.027 || best acc : 99.95%, best loss: 0.0051\n",
      "Epoch[16/40](100/945) || training loss 0.01031 || training accuracy 99.81% || lr [5.000000000000009e-05]\n",
      "Epoch[16/40](200/945) || training loss 0.007816 || training accuracy 99.91% || lr [5.000000000000009e-05]\n",
      "Epoch[16/40](300/945) || training loss 0.004047 || training accuracy 99.91% || lr [5.000000000000009e-05]\n",
      "Epoch[16/40](400/945) || training loss 0.004153 || training accuracy 99.97% || lr [5.000000000000009e-05]\n",
      "Epoch[16/40](500/945) || training loss 0.004415 || training accuracy 99.94% || lr [5.000000000000009e-05]\n",
      "Epoch[16/40](600/945) || training loss 0.004348 || training accuracy 99.91% || lr [5.000000000000009e-05]\n",
      "Epoch[16/40](700/945) || training loss 0.004355 || training accuracy 99.88% || lr [5.000000000000009e-05]\n",
      "Epoch[16/40](800/945) || training loss 0.00554 || training accuracy 99.88% || lr [5.000000000000009e-05]\n",
      "Epoch[16/40](900/945) || training loss 0.00281 || training accuracy 100.00% || lr [5.000000000000009e-05]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 99.97%, loss: 0.005 || best acc : 99.97%, best loss: 0.005\n",
      "Epoch[17/40](100/945) || training loss 0.00284 || training accuracy 99.97% || lr [0.0]\n",
      "Epoch[17/40](200/945) || training loss 0.002176 || training accuracy 99.97% || lr [0.0]\n",
      "Epoch[17/40](300/945) || training loss 0.001952 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[17/40](400/945) || training loss 0.003328 || training accuracy 99.94% || lr [0.0]\n",
      "Epoch[17/40](500/945) || training loss 0.002884 || training accuracy 99.94% || lr [0.0]\n",
      "Epoch[17/40](600/945) || training loss 0.001784 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[17/40](700/945) || training loss 0.001875 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[17/40](800/945) || training loss 0.001923 || training accuracy 100.00% || lr [0.0]\n",
      "Epoch[17/40](900/945) || training loss 0.002403 || training accuracy 100.00% || lr [0.0]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.95%, loss: 0.0051 || best acc : 99.97%, best loss: 0.005\n",
      "Epoch[18/40](100/945) || training loss 0.001997 || training accuracy 99.97% || lr [4.9999999999999996e-05]\n",
      "Epoch[18/40](200/945) || training loss 0.001372 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[18/40](300/945) || training loss 0.00124 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[18/40](400/945) || training loss 0.001753 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[18/40](500/945) || training loss 0.001562 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[18/40](600/945) || training loss 0.001266 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[18/40](700/945) || training loss 0.001329 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[18/40](800/945) || training loss 0.001296 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Epoch[18/40](900/945) || training loss 0.001519 || training accuracy 100.00% || lr [4.9999999999999996e-05]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.85%, loss: 0.0056 || best acc : 99.97%, best loss: 0.005\n",
      "Epoch[19/40](100/945) || training loss 0.003123 || training accuracy 99.97% || lr [0.00010000000000000028]\n",
      "Epoch[19/40](200/945) || training loss 0.02874 || training accuracy 99.03% || lr [0.00010000000000000028]\n",
      "Epoch[19/40](300/945) || training loss 0.0974 || training accuracy 96.75% || lr [0.00010000000000000028]\n",
      "Epoch[19/40](400/945) || training loss 0.08759 || training accuracy 97.50% || lr [0.00010000000000000028]\n",
      "Epoch[19/40](500/945) || training loss 0.05813 || training accuracy 98.34% || lr [0.00010000000000000028]\n",
      "Epoch[19/40](600/945) || training loss 0.04127 || training accuracy 98.88% || lr [0.00010000000000000028]\n",
      "Epoch[19/40](700/945) || training loss 0.03734 || training accuracy 98.91% || lr [0.00010000000000000028]\n",
      "Epoch[19/40](800/945) || training loss 0.02628 || training accuracy 99.47% || lr [0.00010000000000000028]\n",
      "Epoch[19/40](900/945) || training loss 0.0171 || training accuracy 99.62% || lr [0.00010000000000000028]\n",
      "Calculating validation results...\n",
      "[Val] acc : 99.56%, loss: 0.022 || best acc : 99.97%, best loss: 0.005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-bb67b7a1f2c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mmatches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrain_log_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n",
    "\n",
    "counter = 0\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "train_log_interval = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train loop\n",
    "    model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for idx, train_batch in enumerate(train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outs = model(inputs)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # -- Gradient Accumulation\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % train_log_interval == 0:\n",
    "            train_loss = loss_value / train_log_interval\n",
    "            train_acc = matches / batch_size / train_log_interval\n",
    "            current_lr = scheduler.get_last_lr()\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"Calculating validation results...\")\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        for val_batch in val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = criterion(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(val_dataset)\n",
    "        \n",
    "        # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        if val_acc > best_val_acc:\n",
    "            print(\"New best model for val accuracy! saving the model..\")\n",
    "            torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n",
    "        if counter > patience:\n",
    "            print(\"Early Stopping...\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc423d-d856-4eae-ade8-915c89ec44d7",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11400605-10ea-412d-8770-98c5559d41a9",
   "metadata": {
    "id": "PV5CYMXRS_AE"
   },
   "source": [
    "### 학습된 모델 저장하기\n",
    " - `torch.save(model.state_dict(), save_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c918bf94-ceef-4591-9386-e33a0b971292",
   "metadata": {
    "id": "Cq8GYbN1S_AF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./runs/best.pth 폴더에 모델이 성공적으로 저장되었습니다.\n",
      "해당 폴더의 파일 리스트: ['best.pth']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_folder = \"./runs/\"\n",
    "save_path = os.path.join(save_folder, \"best.pth\")   # ./runs/best.pth\n",
    "os.makedirs(save_folder, exist_ok=True)  \n",
    "\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"{save_path} 폴더에 모델이 성공적으로 저장되었습니다.\")\n",
    "print(f\"해당 폴더의 파일 리스트: {os.listdir(save_folder)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c27ff-62c1-4353-8e69-ae7e5a0f4388",
   "metadata": {
    "id": "BqwVdTsKS_AF"
   },
   "source": [
    "### 저장된 모델 불러오기\n",
    " - model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfff393f-5bc2-480c-b226-7cc0e0ea98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e17086cd-b30d-4771-a77f-f2c8378907c2",
   "metadata": {
    "id": "bZ4UN19tS_AG"
   },
   "outputs": [],
   "source": [
    "save_path = f\"results/{name}/016_accuracy_99.97%.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6b25591-80c8-4d32-9a62-ffe086d254e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/final/016_accuracy_99.97%.ckpt'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6ee9c02-600b-4104-8ad0-9353d3a1f641",
   "metadata": {
    "id": "bZ4UN19tS_AG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/final/016_accuracy_99.97%.ckpt 에서 성공적으로 모델을 load 하였습니다.\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "print(f\"{save_path} 에서 성공적으로 모델을 load 하였습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2deaf794-a8e7-459c-a384-53378d280f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, img_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb4f908b-756e-4feb-a5d7-7534fdcc4813",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval'\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc2142ad-853f-4b36-ba34-071670f1e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4be40-2314-456e-9aeb-fe2374f4b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            Resize(img_size[0], img_size[1]),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b64a5961-9e04-4e6a-9823-2026fabbc729",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([transforms.Resize((512, 384), Image.BILINEAR),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.548, 0.504, 0.479], [0.237, 0.247, 0.246])\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a7b8b5a4-6a28-4896-b827-0fc8574f9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "test_dataset = TestDataset(image_paths)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ab7c393d-eb50-4dab-976f-0bbe2dcaba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission_final2.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7-8_Training_Inference (정답).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
